[{"title":"使用faasd体验轻量级serverless服务","date":"2021-12-20T16:25:26.478Z","url":"/2021/12/21/faasd/","tags":[["FaaS","/tags/FaaS/"],["Kubernetes","/tags/Kubernetes/"]],"categories":[["FaaS","/categories/FaaS/"]],"content":"faasd简介FaaS这个词相信大家有所耳闻,而OpenFaaS就是一款相当流行的FaaS框架,提供了多个版本,可以在kubernetes上运行,也能在其他部署了容器运行时的地方运行,faasd是他的轻量级的serverless,方便到可以在你的树莓派上体验serverless。 部署faasd官方提供了脚本,实际上一键执行即可,一般不会有什么问题,如果你使用CentOS7这种比较老旧的系统的话,推荐你先升级内核再进行部署。升级内核的方法我说过很多次,不再赘述。 没错 这就部署完成了,是不是非常简单。 过一会他会弹出一些信息，你可以获取到你的用户名和密码,存放在/var/lib/faasd/secrets/下。 目前来看试了几次都没出什么问题,唯一可能造成影响的问题应该是网络问题，国内访问github不太稳定,而很多资源需要在github拉取,这个的话可以挂代理也可以自己手动去部署。 部署完成后,访问对应服务器的8080端口就可以看见openfaas的ui了。 查看容器相关信息: 基本使用faas-cli工具faasd部署完成后,就可以使用faas-cli工具了。 这个工具也可以在其他机器上使用,连接远程的openfaas(需要先执行export OPENFAAS_URL=) 因为web-ui比较简单,所以这边就不提了。 使用前需要先进行登陆操作: 简单部署一个服务测试一下： 可以看到很快就部署ok 动手部署一个自己的faas服务当然不可能自己从头开始自己做,可以看看提供的模板： 部分模板不是官方的,如我们现在准备用的rust openfaas的模板列表类似于helm的镜像仓库,它提供各种模板 拉取也比较简单: 拉取完成后可以看到当前目录下有一个template文件夹 创建faas-cli新项目: 此时目录结构: 为了做区分,稍微修改一下rust函数返回的内容（test/lib.rs),这个函数会在我们在body中发送的内容前面添加hello 并且返回。 如果你是在其他机器上远程连到faasd,并且机器上有docker，那么可以直接使用faas-cli build -f test.yaml来构建并推送镜像,这边由于是同一台机器,所以采用kaniko来构建,标准的构建上下文通过faas-cli build --shrinkwrap -f test.yml生成: PS: 这边为了省力使用了阿里云的镜像仓库,也可以使用自建的私有仓库,参考此处 推送完成后手动部署 简单测试: OK，非常完美 参考链接 "},{"title":"来自Deis工作室的新玩具-Krustlet","date":"2021-11-14T05:15:04.485Z","url":"/2021/11/14/%E6%9D%A5%E8%87%AADeis%E5%B7%A5%E4%BD%9C%E5%AE%A4%E7%9A%84%E6%96%B0%E7%8E%A9%E5%85%B7-Krustlet/","tags":[["kubernetes","/tags/kubernetes/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"简单介绍大家都知道,Kubernetes 正在成为一个统一的调度器,只要符合kubernetes的标准,那么就可以通过它来进行调度,比如说kubevirt(调度虚拟机),当然我们今天不是为了调度虚拟机来的,我们需要调度的是WASM。 Krustlet和helm同出一源,使用rust实现kubelet,用于在kubernetes中运行wasm,只不过成熟度还是相对不足,不过作为一个概念性的尝试,确实很有意思。 部署部署其实比较简单,按照官方文档按部就班的部署就可以,简单的执行一下 部署一个能用的kubernetes集群这个集群能用就行,也包括kind,minikube,microk8s等 这边图省事,就用微软官方视频里介绍的kind方案好了,同样也可用于其他的k8s集群。 kind的yaml文件(cluster.yaml): 这个yaml文件会定义一个单master双worker的kubernetes集群 简单执行: 看一下集群状态: 记一下宿主机的ip地址: 准备bootstrap文件安装一下kubectl,拷贝一份kubeconfig,用户须有权限操作kube-system的Secret并且能审批CertificateSigningRequests, 这边我们用默认的就好。 执行官方提供的脚本官方提供了一个脚本,用于帮助我们生成bootstrap.conf 这个脚本在官方仓库的script目录下 拷贝kubeconfigcp ~/.kube/config ~/.krustlet/config/kubeconfig 安装krustlet根据自己操作系统的版本选择,下载地址 krustlet 在部分系统可能存在的问题因为图顺手用的是CentOS7的系统,这个系统相对来说比较老,krustlet部署时会出现一些问题 启动krustlet 启动后会提示你kubectl certificate approve &lt;hostname&gt;-tls,我们另起一个终端, 一些问题用kind启动集群时 大概是会出现这种报错。 官方的issue也有这个问题,参考链接 无需介意,kind的集群会尝试在krustlet机器上部署kindnet。因为我们的机器没有运行容器的环境,所以尝试会一直失败,问题不大.。 运行一下官方提供的mod kubectl apply --filename= k8s.yaml的内容: 文件内容 很快这个pod就执行完任务退出了: 看一眼输出: 不过krustlet的镜像和普通的镜像不同,需要用专门的工具,如果你想从头到尾体验一遍,可以参考这个项目"},{"title":"部署一个比较靠谱的prometheus监控","date":"2021-10-26T12:21:41.745Z","url":"/2021/10/26/Promethues/","tags":[["kubernetes","/tags/kubernetes/"],["prometheus","/tags/prometheus/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"部署方案以及组件当前helm版本:3.6.3 Prometheus charts 版本: 14.11.0 grafana charts 版本：6.17.2 此处采用helm部署,helm提供的组件如下: alertmanager: 负责接收并处理来自Prometheus Server(也可以是其它的客户端程序)的告警信息。Alertmanager可以对这些告警信息进行进一步的处理，比如当接收到大量重复告警时能够消除重复的告警信息，同时对告警信息进行分组并且路由到正确的通知方，Prometheus内置了对邮件，Slack等多种通知方式的支持，同时还支持与Webhook的集成，以支持更多定制化的场景. nodeExporter: Node Exporter 是 Prometheus 官方发布的，用于采集节点的系统信息，比如 CPU，内存，磁盘和网络等信息。 pushgateway: Prometheus Pushgateway 允许将时间序列从短暂的服务级别批处理作业推送到 Prometheus 可以采集的中间作业。结合 Prometheus 的基于文本的简单展示格式，即使没有客户端库，也可以轻松集成，甚至编写 shell 脚本。 server: prometheus 主程序 从helm仓库下载helm chart 打开prometheus文件夹下的values.yaml,修改以下部分: 修改kube-metrics镜像由于k8s.gcr.io在国内无法访问,所以无法使用原有的所有镜像,据说阿里云提供gcr.io镜像仓库的镜像,但是经过实际测试,缺少大量镜像,目前的解决方案是使用bitnami/kube-state-metrics镜像替代原有的镜像。 修改prometheus/charts/kube-state-metrics/values.yaml 修改prometheus.yaml的内容(非必要)由于准备采用1 K8S for Prometheus Dashboard 20211010 编号 13105的grafana模板来展示数据。所以需要对job相应的修改,如果不准备采用此模板,可不做此项 关于节点名称的标签，因为cadvisor是使用instance，而kube-state-metrics是使用node；这样会导致grafana节点信息表格中，没有统一的字段来连接各个查询，所以需要在prometheus.yaml中增加以下内容来给cadvisor的指标复制一个node标签。 这边直接在values.yaml修改以下部分: 部署 prometheus 到集群简单执行: 部署grafana 作为dashboard拉取helm charts简单执行: 修改grafana/values.yaml, 添加grafana监控dashboard可在此处找所需的监控模板,这边使用13105 添加数据源填写prometheus的service的地址和端口 导入dashboard输入13105 点击load点击左侧面板的Search Dashboards 可找到刚才的dashboard 一些问题目前使用他直接提供的dashboard似乎会存在某些问题 如安装prometheus时需添加node的label 容器网络显示也会出现问题,会出现nodata的情况 解决方案： 编辑nodata的这部分带宽数据 修改name = ~&quot;^k8s_.*&quot; 为name=~&quot;.*&quot;"},{"title":"在私有云部署阿里云NAS存储","date":"2021-10-20T10:55:59.208Z","url":"/2021/10/20/nas/","tags":[["kubernetes","/tags/kubernetes/"],["pv","/tags/pv/"],["pvc","/tags/pvc/"],["storageclass","/tags/storageclass/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"前提条件私有云内网与阿里云的VPC互通 配置文件下载首先下载阿里云提供的alibaba-cloud-csi-driver 在deploy文件夹下,找到rbac.yaml,这个文件用于配置csi插件的rbac权限, 找到deploy/nas文件夹,其中配置文件为nas-plugin-dedicated.yaml以及nas-provisioner-dedicated.yaml 注意: 原文件夹中有两套不同的配置,根据阿里云官方的解释,在私有云采用阿里云的NAS应该使用dedicated 修改部分配置要在私有云使用阿里云的csi插件,还需要声明csi-node的nodeid,在非阿里云的机器中,我们需要额外设置hostname作为nodeid 配置方法 nas-plugin-dedicated.yaml: nas-provisioner-dedicated.yaml： 安装CSI插件 kubernetes 1.20版本后适配问题在较高版本(1.20+)的kubernetes中使用阿里云的CSI插件会出现selfLink was empty, can&#39;t make reference报错,原因是kubernetes 1.20版本后禁用了SelfLink 根据工单,阿里云官方的解释是CSI插件 1.1.4以后的版本适配1.20 当时的解决方案 在kube-apiserver 的yaml文件中添加启动参数--feature-gates=RemoveSelfLink=false,重新apply后生效 使用阿里云NAS作为storageClass 参数 描述 mountOptions 挂载NAS的options参数在mountOptions中配置，包括NFS协议版本。 volumeAs 可选subpath、filesystem，分别表示创建子目录类型的PV和文件系统类型PV。 server 表示创建子目录类型的PV时，NAS文件系统的挂载点地址。 provisioner 驱动类型。本例中取值为nasplugin.csi.alibabacloud.com，表示使用阿里云NAS CSI插件。 reclaimPolicy PV的回收策略，默认为Delete，支持Retain。Delete模式：删除PVC的时候，PV和NAS文件系统会一起删除。Retain模式：删除PVC的时候，PV和NAS文件系统不会被删除，需要您手动删除。如果数据安全性要求高，推荐使用Retain方式以免误删数据。 archiveOnDelete 表示在reclaimPolicy为Delete时，是否删除后端存储。因为NAS为共享存储，添加此选项进行双重确认。在参数parameters下配置。默认为true，表示不会真正删除目录或文件，而是将其Rename，格式为：archived-&#123;pvName&#125;.&#123;timestamp&#125;；如果是配置为false，表示会真正删除后端对应的存储资源。 "},{"title":"使用kube-vip+containerd在openstack上搭建集群","date":"2021-09-23T12:01:03.407Z","url":"/2021/09/23/kube-vip/","tags":[["kubernetes","/tags/kubernetes/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"环境准备本文档仅做演示,除搭建master外其余操作与普通k8s集群相同,故省略部分操作 当前使用机器的操作系统均为CentOS7 主机名称 ip地址 备注 K8s-master-1 10.1.12.217 master节点 K8s-master-2 10.1.12.169 master节点 api.k8s.local 10.1.12.218 master(虚拟浮动IP) 环境初始化 内核升级内核升级的原因以及版本选择请看文末QA 依赖服务安装 开启时间同步: 容器运行时Containerd较多用户可能采用tar安装的方式安装containerd,本人相对较懒,由于docker-ce中也采用containerd,故直接安装docker-ce提供的软件包即可 修改containerd配置文件： 安装k8s 初始化前准备安装完kubernetes的组件后,crictl命令就可以使用了,但是我们发现实际使用会报错，原因是没有修改containerd的sock路径 修改kubelet启动配置,配置cgroup为systemd,否则会出现hostname无法解析的错误 Kube-vip 配置 openstack配置注意! 此处非必需 openstack的网络类型下,需要手动调整网络端口,在网络–&gt; 子网-&gt;端口中找到相应机器的端口 添加额外ip地址,添加后虚拟ip即可生效 启动k8s 模板文件 Master1 初始化 Master2 加入节点 加入Node 添加cni插件 此时所有的节点都会变成ready状态 后记部分QAQ: 为什么要升级内核 A: 以CentOS7为例,它的内核版本为3.10,3.10版本内核的release日期是2013年,且不说k8s,Docker容器开始进入视野也是13年以后的事情,并且随着Docker逐步更新,对内核版本的依赖也逐渐加强,许多高级特性都需要较高版本的内核才能使用,如Docker的Linux CGroup memory 在4.0以下版本的内核表现并不稳定 Q: 内核版本如何选择 A: 同样以CentOS为例,添加了elrepo之后 通常会给出新旧两个版本的内核可供升级,个人倾向于相对老版本的内核,稳定性更佳。如现在(2021-09-23),执行查询后可看到内核版本共两个, 相对较老的版本为5.4 以下为google GKE的节点内核版本 Anthos clusters on VMware 版本 Kubernetes 版本 节点内核版本 1.8.2 v1.20.9-gke.701 5.4.0.1021.22 (ubuntu)、5.4.120-r118 (cos) 1.8.1 v1.20.8-gke.1500 5.4.0-1018.19 (ubuntu)、5.4.120-r116 (cos) 1.8.0-gke.25 1.20.5-gke.1301 5.4.0.1015.16 (ubuntu)、5.4.104-r99 (cos) 1.8.0-gke.21 1.20.5-gke.1301 5.4.0.1015.16 (ubuntu)、5.4.104-r99 (cos) 1.7.3-gke.6 v1.19.12-gke.1100 5.4.0.1021.22 (ubuntu)、5.4.120-r118 (cos) 1.7.3-gke.2 1.19.12-gke.1100 5.4.0.1021.22 (ubuntu)、5.4.120-r118 (cos) 1.7.2 1.19.10 5.4.0.1015.16 (ubuntu)、5.4.104-r99 (cos) 1.7.1 1.19.7 5.4.0.1014.15 (ubuntu)、5.4.104-r97 (cos) 1.7.0 1.19.7 5.4.0.1010.11 (ubuntu)、5.4.92 (cos) 1.6.4 1.18.20 5.4.0.1021.22 1.6.3 1.18.18 5.4.0.1014.15 1.6.2 1.18.13 5.4.0.1009.10 1.6.1 1.18.13 5.4.0.1007.8 1.6.0 1.18.6 5.4.0.1004.5 k8s版本选择参考EKS(AWS推出的kubernetes平台)每个小版本后第六个修补版本为相对稳定性强的版本 如1.14.6 1.15.6 为什么采用containerd去docker化是kubernetes发展的大势所趋,采用containerd更符合时代需要 参考链接: 木子的笔记 使用kube-vip搭建高可用kubernetes集群"},{"title":"ansible中遇上的一个小问题","date":"2021-08-31T10:42:34.000Z","url":"/2021/08/31/ansibleproblem/","tags":[["故障处理","/tags/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"],["ansible","/tags/ansible/"]],"categories":[["Linux","/categories/Linux/"]],"content":"今天在新工作的地方研究了一下自动化的playbook,刚好遇上一个很奇怪的问题,来自ansible(version=2.10.1) 问题描述由于需要利用ansible调用zabbix的JMX监控,而监控的触发器在不同环境又是不一样的，因此需要利用ansible的set_fact 模块,为不同的环境添加不同的模板。 同时ansible 需要支持自定义传参添加额外监控模板功能。以下是实现： 举个例子: 看起来很美好对吧 那就大错特错了。 实际执行起来这段playbook会被跳过 原因排查查来查去发现是因为加了with_item的原因, 由于item的默认值是一个空的列表inputParamZabbixTemplate: [] 实际上就不会对他进行操作。 解决办法解决办法其实比较简单，就是在这一步之前执行一个变量初始化操作，这样如果有item的话原有逻辑也能继续执行，同样的如果没有的话跳过，也能依照初始变量运行 "},{"title":"利用Earthly工具进行CI/CD","date":"2021-07-22T11:00:45.025Z","url":"/2021/07/22/earthly/","tags":[["DevOps","/tags/DevOps/"]],"categories":[["docker","/categories/docker/"]],"content":"Earthly工具尝试最近发现一个比较有意思的镜像构建工具Earthly,文档地址 根据Earthly工具的介绍,它相当于Dockerfile+MAkefile+Bash 简单部署根据官网文档简单试用一下： 因为各种原因，个人选择在docker:dind的容器环境中尝试使用。 启动docker:dind容器： 在容器中： 官网例子 Earthfile: hello.py: 执行build部分： 执行docker部分 Earthly + Gitlab-CI简单实践备注：此处仅做演示,故仅采用docker:dind容器来进行CI,仓库仍采用上面的python演示仓库 Earthfile .gitlab-ci.yml 等待执行完毕即可,生产环境清使用专有earthly镜像"},{"title":"在阿里云ack中部署Yapi","date":"2021-07-22T10:48:19.309Z","url":"/2021/07/22/yapi-ack/","tags":[["kubernetes","/tags/kubernetes/"],["DevOps","/tags/DevOps/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"昨天遇上的需求，需要在公网环境部署一个Yapi服务,用于接口测试。做个小记录。 参考了此处 构建Yapi镜像搜索之后发现Yapi官方并没有提供镜像,于是参考上面的文档自己制作了一个镜像,过程参考上面的文档即可。 部署mongodb这边为了方便,使用helm部署。 从helm仓库下载helm chart 修改参数这边修改了以下参数： 部署 建立Yapi账号和数据库直接进pod执行，也没啥好说的 安装Yapi我们刚才已经制作了一个Yapi的镜像，为了方便使用，将其推送到公网的的自建harbor上。 使用kustomize来部署 kustomization.yaml deployment.yaml secret.yaml service.yaml ingress.yaml 部署"},{"title":"从python脚本到skopeo 容器搬运篇(2)","date":"2021-07-21T10:39:29.874Z","url":"/2021/07/21/imagetrans/","tags":[["docker","/tags/docker/"]],"categories":[["docker","/categories/docker/"]],"content":"在生产中有时候会遇上一个比较恶心的问题，因为经常性需要去各个甲方部署，公司的平台基于docker-compose部署，另外还有大大小小各种乱七八糟几百个镜像的业务依赖，每部署一次都需要重新挑出所需的镜像对其进行导出，再将其部署。 原有方案将docker镜像通过docker save 的方式导出为tar包，然后再到需要部署的机器上导入： 问题这个方案存在两个问题，一来是docker容器通过docker save 导出后会占用大量的磁盘空间，而且费时费力，另外需要在某台机器先部署好镜像才能进行导出， 为了提高效率，拟采用新的方式进行镜像的搬运。 使用私有镜像仓库来进行搬运docker 官方镜像中存在docker registry的镜像，该镜像可比较方便的进行导入导出，并且镜像分层，可以较好的解决磁盘占用和导入时间长的问题。这个方式类似上一章中提到的harbor做镜像存储操作， 此时registry文件夹即保存的镜像， 部署： 通过python脚本简单下载： 问题这个方案虽然解决磁盘占用问题，但仍需要通过本地的docker 导出镜像。同样会带来大量的镜像。 最终方案(?)最近听说了skopeo之后发现它可以比较方便的解决这个问题，但执行起来还是有一些阻碍： 各种依赖镜像原本都是使用Dockerfile进行构建的,此时使用skopeo去获取镜像并不容易 skopeo保存的镜像仍然存在分层重复的问题，重复占用磁盘的问题仍旧存在。 解决问题： 参考了此处 搭建私有的harbor仓库不多做赘述,可以参考此处 在本地构建镜像后推送到私有harbor先构建全部的可能用到的镜像，构建过程略，在harbor建立一个image仓库，然后推送： 使用skopeo拉取镜像skopeo镜像拉取方式： 通过脚本获取需要的镜像列表，然后对其分别拉取到image目录。 此时image的目录结构如下： 转换拉取的镜像文件 导出后文件夹格式： 拉取镜像测试"},{"title":"从python脚本到skopeo 容器搬运篇(1)","date":"2021-07-19T13:53:33.395Z","url":"/2021/07/19/skopeo1/","tags":[["docker","/tags/docker/"]],"categories":[["docker","/categories/docker/"]],"content":"镜像搬运的必要性由于业务原因经常有要求对镜像分组管理的要求，但很多镜像原本是重复的,并且都是单独的镜像，无法以单独的compose或者其他形式管理，要跨机器搬运就比较困难，原有的做法是新建一个临时的harbor仓库，做一个中转。 原有的搬运操作原有的一些镜像是存储在单独的一台服务器上，以docker image形式存在的,所以只进行了打包和推送操作: 在另外一台机器上部署： skopeo的使用和操作安装参照此链接 进行安装 基本使用 登陆私有仓库 拷贝到仓库： "},{"title":"kubernetes学习笔记(tekton)---基本部署","date":"2021-07-01T16:25:07.775Z","url":"/2021/07/02/kubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(tekton)/","tags":[["kubernetes","/tags/kubernetes/"],["DevOps","/tags/DevOps/"],["tekton","/tags/tekton/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"简介Tekton是一个云原生的CI/CD构建框架。 基本概念Task: Tekton中最小的单位，一切任务都是由Task组成。 TaskRun：创建了一个TaskRun以后，每个TaskRun就会单独控制一个Pod，Task中的每一个Step都对应pod中的一个Container。 Pipeline： Pipeline由多个Task组成，多个Task串联，按顺序执行就组成了Pipeline。 PipelineRun ： PipelineRun的作用为实际执行Pipeline。 PipelineResource: PipelineResource是Pipeline上的各种输入输出资源，比如github上的源码(输入)，编译产物或者容器镜像(输出)。 ClusterTask: 是覆盖整个集群的任务，可以在集群的范围运行任务，而不是和Task一样只能在某一命名空间运行。 运行机制每当我们创建一个Pipeline，他就会读取其中的各个Task任务,执行PipelineRun时实际开始运行Pipeline，每次运行都会生成一个新的PipelineRun，它会产生对应的TaskRun,由TaskRun去产生对应的Pod，运行任务。 部署部署Tekton TekTon 国内可能存在镜像拉取失败问题，需要魔法方法解决。 部署Tekton的dashboard ，可以方便直观的管理Tekton 部署tkn工具tkn是Tekton的CLI工具，在github直接下载对应版本即可，使用tkn可以很方便的管理Tekton 基本部署就说这么多，下一节再研究基本使用"},{"title":"Docker连接harbor出现x509的处理办法","date":"2021-07-01T10:44:05.004Z","url":"/2021/07/01/Docker%E8%BF%9E%E6%8E%A5harbor%E5%87%BA%E7%8E%B0x509%E7%9A%84%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95/","tags":[["docker","/tags/docker/"],["harbor","/tags/harbor/"],["Linux","/tags/Linux/"],["故障处理","/tags/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"]],"categories":[["docker","/categories/docker/"]],"content":"今天在使用tekton做gitops时出现了x509的错误，kubernetes中的pod无法访问本地的harbor，出现这个问题的原因是本地的harbor用的是自签证书，导致docker访问时不信任。 原先的解决办法： 在/etc/docker/daemon.json添加 &quot;insecure-registries&quot;:[&quot;$DOMAIN/IP&quot;] ，然后重启docker。 但是由于不想关停测试用的kubernetes，这个办法就不可行了，于是采取了其他方式。 首先是将harbor的证书拷贝到/etc/pki/ca-trust/source/anchors/下 ，发现没有作用。 然后采取了如下方式。 在/etc/docker/certs.d(需要自建)新建一个文件夹 名称为harbor域名(e.g:www.example.com ),拷贝harbor的crt证书到此处。 成功运行tekton的task"},{"title":"定时任务(cron与at)","date":"2021-06-29T11:51:20.181Z","url":"/2021/06/29/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1(cron%E4%B8%8Eat)/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"cron(定时任务)cron是最常用的定时任务类型,用于执行周期性计划 对应的服务为crond，使用前请确保此服务已运行 常用命令crontab -e 使用vi文本编辑器编辑cronjob，可通过-u指定用户执行。 crontab -l 列出所有cronjob: crontab -r 删除用户的cronjob cronjob 格式 前五个*分别代表 分、时、日、月、周，最后一列是需要执行的命令。 使用L 可代表本月最后一天/本周最后一天： e.g: 在天（月）子表达式中，“L”表示一个月的最后一天在天（星期）自表达式中，“L”表示一个星期的最后一天，也就是SAT6L”表示这个月的倒数第６天，“FRIL”表示这个月的最一个星期五在使用“L”参数时，不要指定列表或范围，因为这会导致问题 例1：需要在每周五晚上三点钟执行 /root/dump.sh 脚本 例2：需要在每月10号执行 /root/dump.sh脚本 例3：在每月月底最后一天执行 /root/dump.sh脚本 cron的权限管理/etc/cron.deny文件为cron的黑名单，在此名单中的用户无法执行cron 另外/etc/cron.allow默认不存在，为白名单，若手动创建，则黑名单失效，除白名单外的用户无法执行cron at命令与周期性任务cron不同，at命令用于执行在某一时间只执行一次的一次性任务。 使用方法： at + 选项 + 时间参数 选项 常用的时间格式：&lt;时&gt;:&lt;分&gt; &lt;年&gt;-&lt;月&gt;-&lt;日&gt; &lt;时&gt;:&lt;分&gt; MMDDYY MM/DD/YY MM.DD.YY 英文月名 日期 年份 各种简写： Midnight(00:00) Noon(12:00) Teatime(16:00), now+time(从现在起的某段时间后执行) 例1:五分钟后执行a.sh 写法2： 权限控制同cron， 文件名/etc/at.deny、/etc/at.allow"},{"title":"kubernetes学习笔记(prometheus)","date":"2021-06-28T12:19:12.945Z","url":"/2021/06/28/kubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(prometheus)/","tags":[["kubernetes","/tags/kubernetes/"],["prometheus","/tags/prometheus/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"安装prometheusRBAC授权首先使用RBAC对prometheus进行授权: prometheus存储定义StorageClass存储（需要读写权限）: 配置文件我们把配置文件放在configMap中进行管理: 部署prometheus 注:这边要采用ingress的方式访问prometheus,也可通过其他方式访问: 方式二: 127.0.0.1:9090可以访问到prometheus 暴露prometheus服务 访问测试修改本地hosts为prometheus.k8s.local 访问此地址即可。 部署GrafanaGrafana是一个相对好用的数据展示工具，部署起来也比较简单 定义存储 部署Grafana 暴露服务 部署node-exporternode-exporter用于收集节点信息 使用在Grafana的dashboard中添加data source, 填入prometheus地址 ,这边是，刷新即可看到效果"},{"title":"通过例子学rust(迷你区块链)","date":"2021-06-24T15:37:53.922Z","url":"/2021/06/24/%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E5%AD%A6rust(%E8%BF%B7%E4%BD%A0%E5%8C%BA%E5%9D%97%E9%93%BE)/","tags":[["rust","/tags/rust/"]],"categories":[["rust","/categories/rust/"]],"content":"依旧来自 导入所需的包Cargo.toml 创建结构体(blockchain.rs)交易信息交易结构体需要包含发送者，接收者以及金额，同时需要实现序列化,Debug和Clone(后续创建新块需要)特性 区块头包含时间戳,前一个区块的hash,难度,默克尔值和nonce。 其中默克尔值是一个hash值，它包含每一笔交易的hash值。 nonce是用于工作量证明的值，得到一个nonce使得区块头的hash前X位为0，其中X为难度 区块体包含区块头，交易计数以及所有的交易信息 链包含区块列表当前交易，难度，矿工地址以及奖励 为链创建方法(blockchain.rs)创建新链要创建一个全新的区块链，我们需要得到第一笔交易信息，由Root发给第一位矿工。 首先新建一个链，接收矿工地址和难度两个参数，调用generate_new_block()方法生成第一个区块，最后返回一个chain实例 修改难度和奖励创建修改难度和奖励的方法 创建新的交易创建新交易很简单，只需要将发送者和接受者信息传入Transaction结构体，最后将结构体加入到链中的交易信息里即可，返回的bool用于判断是否创建成功。 计算当前块的hash值定义last_hash方法来计算当前块的hash值。 创建新的区块首先生成区块头，传入初始化信息。原视频使用time生成时间戳,这边使用了chrono。 然后产生一笔新的交易，从Root给miner。 创建一个新的区块，将此处的新交易添加到区块的交易中，并且将历史交易记录添加到此区块，此时transaction列表长度即为交易数。 通过get_merkle方法得到默克尔值，通过proof_of_work得到nonce，最后将此block打包到链中 计算默克尔值传入当前交易的列表，取出每一笔交易 计算hash后加入merkle列表。 如果默克尔列表中有奇数个值，则复制最后一个值并将其加入默克尔列表。 从默克尔hash列表中 取出前两个交易的hash值，拼接后将调用hash()得到一个hash，重复此操作直到默克尔列表中的hash被合并为一个时pop并返回。 工作量证明接收一个区块头并且计算hash，从hash中取出一定长度的切片并且将其解析，其中长度由difficulty决定。 parse::&lt;u32&gt;()会尝试将其解析为u32类型，这边得到的是一个Result类型，我们的目的是得到一个nonce使得最终区块头的hash值中，前X位都是0，所以在得到ERROR时，我们依然将nonce+1，知道得到所需的hash为止。 最后我们就得到了一个可以将区块头的hash的前X位变成0的nonce，用于证明工作。 hash计算hash接受一个可被serde序列化的结构体，将结构体变成字符串，我们调用sha2的方法输出得到一个&amp;[u8]类型的列表，通过hex_to_string 将其转换为一个16进制字符串，这边使用std::fmt::Write实现 主函数调用(main.rs)启动初始化启动此程序时，我们需要初始化数据，开始第一笔交易以创建区块链， 使用io::stdin().read_line读取用户输入，并且将参数传给blockchain::Chain::new创建全新的区块链 循环内容打印菜单，使用match匹配用户输入，对其做相应处理，通过对应函数返回的bool来判断执行结果 效果 完整代码 点击这里 blockchain.rs main.rs "},{"title":"kubernetes学习笔记(helm)","date":"2021-06-23T10:40:28.357Z","url":"/2021/06/23/helm%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["kubernetes","/tags/kubernetes/"],["helm","/tags/helm/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"安装helm我们可以使用以下命令安装helm 直接执行： 备注: 虽然存在包管理器安装，但官方并不建议使用 helm 基本功能1.创建新的 chart2.chart 打包成 tgz 格式3.上传 chart 到 chart 仓库或从仓库中下载 chart4.在Kubernetes集群中安装或卸载 chart5.管理用Helm安装的 chart 的发布周期 helm基本构成helm三大概念：Chart 代表着 Helm 包。它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。你可以把它看作是 Homebrew formula，Apt dpkg，或 Yum RPM 在Kubernetes 中的等价物。 Repository（仓库）是用来存放和共享 charts 的地方。它就像 Perl 的CPAN 档案库网络或是 Fedora 的软件包仓库,只不过它是供 Kubernetes 包所使用的。 Release是运行在 Kubernetes集群中的 chart 的实例。一个chart通常可以在同一个集群中安装多次。每一次安装都会创建一个新的Release。以 MySQL chart为例，如果你想在你的集群中运行两个数据库，你可以安装该chart两次。每一个数据库都会拥有它自己的release和 release name。 在了解了上述这些概念以后，我们就可以这样来解释 Helm： Helm 安装charts到 Kubernetes 集群中，每次安装都会创建一个新的release。你可以在Helm的 chart repositories中寻找新的 chart。 helm安装chart示例以nginx为例：直接执行helm install安装 查看helm chart支持的参数 以下是完整内容 修改可配置选项在config.yaml 中自定义所需的选项 删除"},{"title":"kubernetes学习笔记(storage)","date":"2021-06-23T10:39:52.444Z","url":"/2021/06/23/k8s%20%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95(3)%20pv&pvc&StorageClass/","tags":[["kubernetes","/tags/kubernetes/"],["pv","/tags/pv/"],["pvc","/tags/pvc/"],["storageclass","/tags/storageclass/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"存储相关概念pv： 持久化卷，可以使用ceph和nfs等pvc: 持久化卷声明,用于调度pv资源StorageClass： 定义存储类型，动态分配存储资源 环境准备首先建立一个nfs的server，此处不做多讲，kubernetes节点上也安装nfs服务 pv相关属性： pv相关属性包括了Capacity(存储能力)、AccessModes(访问模式)、ReclaimPolicy(回收策略)。 基本的capacity指标为storage=”存储容量”。访问模式包含以下三种：ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 注: 不同的存储方式支持的访问模式不同，请参阅相关指南。回收策略： Retain（保留）- 保留数据，需要管理员手工清理数据 Recycle（回收）- 清除 PV 中的数据，效果相当于执行 rm -rf /yourdir/* Delete（删除）- 与 PV 相连的后端存储完成 volume 的删除操作 pv的状态通常有以下几种：Available（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PV 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 pv实践接下来新建pv对象(pv1.yaml)： 应用: 创建pvc 注: pvc会自动寻找available状态的pv，无需额外声明。如果pv为2Gi，pvc为1Gi，同样会进行绑定，且pvc的容量会变成2Gi。 使用pvc作为服务的存储 nginx-service.yaml nginx-ingress.ymal StorageClass实践新建nfs-client的deployment 创建nfs-client的serviceaccount： 此处新建了一个serviceaccount同时绑定了一个clusterrole（用于声明权限） 创建sc对象 使用StorageClass 创建服务 "},{"title":"通过例子学rust(端口嗅探器)","date":"2021-06-19T06:48:35.994Z","url":"/2021/06/19/rust%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E5%AD%A6rust(%E7%AB%AF%E5%8F%A3%E5%97%85%E6%8E%A2%E5%99%A8)/","tags":[["rust","/tags/rust/"]],"categories":[["rust","/categories/rust/"]],"content":"简介个人推荐使用rust的clap库实现命令行程序功能 非原创,原版在这 搬运自 首先分析需求： 获取参数我们使用std::env 来获取用户输入的参数 测试一下得到的结果： 建立结构体我们用结构体来这些参数并且将其实例化 main函数中的错误处理让我们回到main函数 main中执行多线程扫描的部分 scan 函数 完整代码 完整代码 "},{"title":"rust学习笔记(泛型)","date":"2021-06-14T06:46:44.743Z","url":"/2021/06/14/rust%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E6%B3%9B%E5%9E%8B)/","tags":[["rust","/tags/rust/"]],"categories":[["rust","/categories/rust/"]],"content":"学习内容来自B站：原子之音 泛型结构体 结构体方法中的泛型 为泛型结构体中特定的类型实现方法 泛型方法 "},{"title":"kubernetes学习笔记（ingress）","date":"2021-06-09T10:34:58.207Z","url":"/2021/06/09/k8s%20%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95%EF%BC%882%EF%BC%89%20ingress/","tags":[["kubernetes","/tags/kubernetes/"],["ingress","/tags/ingress/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"上次部署完成了k8s的基本框架，现在开始部署ingress，ingress其实就是从 kuberenets 集群外部访问集群的一个入口，将外部的请求转发到集群内不同的 Service 上，其实就相当于 nginx、haproxy 等负载均衡代理服务器。 创建traefik的crd资源 创建rbac.yaml 创建cokfigmap traefik 安装前准备在部署traefik之前 还需要安装Service APIs 安装api的两种方式 网络不佳的情况下 推荐下载下所有文件 创建deploy.yaml 文件 如果需要自定义标签的话 至此 traefik部署完成，如果需要访问traefik的dashboard 部署dashboard 修改本地hosts文件 访问 即可看到dashboard"},{"title":"kubernetes学习笔记（部署）","date":"2021-06-07T16:00:00.000Z","url":"/2021/06/08/k8s%20%20%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/","tags":[["kubernetes","/tags/kubernetes/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"准备三台干净的服务器，系统版本： CentOS Linux release 7.9.2009 添加hosts 配置docker加速 部署master节点 部署node节点 部署cni 容器网络 也可使用flannel 如果要切换cni插件，需要先把所有节点的/etc/cni/net.d/文件清空 验证kubernetes"},{"title":"Hello World","date":"2021-06-05T06:30:57.718Z","url":"/2021/06/05/firstblog/","tags":[["杂谈","/tags/%E6%9D%82%E8%B0%88/"]],"categories":[["杂谈","/categories/%E6%9D%82%E8%B0%88/"]],"content":"各种备忘的以及乱七八糟的东西都放到这从rust,python 的学习笔记到linux,docker 等有机会大概都会在这更新 做个纪录顺便防止自己哪天忘了 hexo主题来源为说明文档地址  "}]