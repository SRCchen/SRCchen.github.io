[{"title":"【译】使用rust来写一个容器(四)","date":"2023-09-17T04:18:16.000Z","url":"/2023/09/17/%E7%94%A8rust%E5%86%99%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A84/","tags":[["rust","/tags/rust/"],["Docker","/tags/Docker/"]],"categories":[["rust","/categories/rust/"]],"content":" 注 本文章为对 Litchi Pi的《Writing a container in Rust》的翻译转载，不享受任何著作权利，不用于任何商业目的，不以任何许可证进行授权，不对任何转载行为尤其是商业转载行为负责。一切权利均由原作者 Litchi Pi 保有。个人翻译能力有限，如有疑问可查看原文。 子进程的诞生在这篇文章中，我们将把我们容器的父进程克隆成一个子进程。在这之前，我们将通过准备一些进程间通信IPC（inter-process communication）通道来做好基础准备，这样就可以与后续创建的子进程进行交互了。 通过sockets进行进程间通信(IPC)IPC 介绍Unix domain sockets 或者叫同主机socket通信是进程间通信（简称IPC）的解决方案，它们与用于与远程主机进行网络操作的”网络socket通信”类型的套接字有所不同。 关于套接字和IPC的的内容你可以在这篇文章中查阅。在实际操作中，它就是一个文件（Unix哲学：一切皆文件）。我们将在这个文件中进行读取或写入，以便将信息从一个进程传输到另一个进程。 对于我们的工具而言。我们并不需要任何复杂的IPC，只希望它能够简单的将布尔值从我们的子进程传入传出即可。 创建socketpair创建一对sockets，一端交给子进程而另一端留给父进程。 这样我们就能够将原始二进制数据从一个进程传输到另一个进程，就像我们将二进制数据写入存储在文件系统中的文件一样。 让我们创建一个包含了所有进程间通信相关内容的新文件src/cli.rs： 我们创建了一个generate_socketpair函数，在这个函数中我们调用了socketpair函数，这是在Unix中创建socket pair的标准方式，只不过我们是通过Rust进行调用的。 AddressFamily::Unix：指定我们使用的是协议族类型为Unix domain socket（详见all AddressFamily variants for details） SockType::SeqPacket：socket将使用具有包和固定长度数据报文的语义通信（Semantic Communication）。（详见all SockType variants for details） None：socket将使用与socket类型相关的默认协议。 SockFlag::SOCK_CLOEXEC：在执行任何exec家族的系统调用后，socket将自动关闭。（详见Linux manual for exec syscalls） 我们使用了新的错误Errcode::SocketError,让我们把它加到src/errors.rs中: 将sockets添加到容器配置中在创建配置时，让我们生成socket pair，将其添加到ContainerOpts数据中，让子进程可以轻松地访问它。修改src/config.rs文件： 同时我们还要修改ContainerOpts::new函数，让它返回构造的ContainerOpts结构体以及sockets，因为父容器需要访问它。 添加到容器实现中,添加清理设置在我们的容器实现中，在Container结构体中添加一个字段来更容易的访问sockets。修改src/container.rs文件： sockets 在进程退出前需要进行清理处理,我们在clean_exit函数中进行关闭sockets相关处理 创建IPC封装为了简化sockets的使用，我们先创建两个封装。由于我们只传输布尔值，所以只需要在src/ipc.rs中创建一个send_boolean和recv_boolean函数： 这里只是与nix库的send和recv函数进行一些交互，处理数据类型转换等…没有太多可说的，但是我们从Rust调用具有低级C后端的函数仍然很有趣。 我们现在暂时不会使用这些封装，但在后面这些它们会很方便。 Patch for this step这一步的代码可以在github litchipi/crabcan branch “step7”中找到.前一步到这一步的原始补丁可以在此处找到 克隆进程为了将所有与克隆和管理子进程相关的内容放在一起，让我们在src/child.rs文件中创建一个新的child模块。首先，在src/main.rs中定义模块： 我们需要创建新的错误类型，用于处理在生成子进程或在容器内部准备过程中出现的问题，接下来将它们添加到src/errors.rs中： 创建子进程现在，我们创建一个虚拟的child函数，简单地打印传入的参数。在src/child.rs中创建这个函数： 这个子进程只会简单打印一些内容到标准输出中，并返回0表示一切正常。我们将希望子进程处理的配置信息全都传入 接下来我们在src/child.rs中创建一个函数，用于克隆父进程并调用子进程： 为了更便于理解, 我们拆分一下这段代码 我们首先分配一个原始数组(缓冲区)，大小为我们定义的STACK_SIZE,也就是1KiB。这个缓冲区用于保存子进程的栈(stack)，请注意，这和C语言中的clone函数不同(细节详见nix::sched::clone文档) 其次，我们将设置需要激活的flags掩码，完整的flags列表及其简单描述可以在nix::sched::CloneFlags文档中找到，或者直接在Linux的clone(2)手册中查看。此处我们跳过flags掩码单独定义，因为它们各自都需要一些适当的解释。 接着，我们调用clone系统调用，将其重定向到我们的child函数，同时带上我们的config结构体，以及进程的临时堆栈和我们设置的flags掩码作为参数。此外，我们还附带了一个指令，要求在子进程退出时向父进程发送SIGCHLD信号。 如果一切顺利，我们将得到一个进程ID，简称PID，这是一个独一无二的数字，用于在Linux内核中唯一标识我们的进程。我们将返回这个PID，将它存储在我们的container结构体中。 关于命名空间的说明如果你不了解什么是linux命名空间,我建议你阅读Wikipedia相关文章来获取快速又全面的介绍。 简单来说，命名空间是Linux内核提供的一种隔离机制，它允许在这个命名空间中的进程拥有与全局不一样的独立系统资源。 Network namespace: 拥有与整个系统不同的网络配置 Host namespace：拥有与整个系统不同的主机名 PID：在命名空间内使用任何PID号，包括init（PID = 1） 其他 你可以查阅linux手册中的namespace部分获取更多信息 译者注 目前Linux的命名空间有： mount：挂载命名空间，使进程有一个独立的挂载文件系统，始于Linux 2.4.19 ipc：ipc命名空间，使进程有一个独立的ipc，包括消息队列，共享内存和信号量，始于Linux 2.6.19 uts：uts命名空间，使进程有一个独立的hostname和domainname，始于Linux 2.6.19 net：network命令空间，使进程有一个独立的网络栈，始于Linux 2.6.24 pid：pid命名空间，使进程有一个独立的pid空间，始于Linux 2.6.24 user：user命名空间，是进程有一个独立的user空间，始于Linux 2.6.23，结束于Linux 3.8 cgroup：cgroup命名空间，使进程有一个独立的cgroup控制组，始于Linux 4.6 容器中通常通过namespace和cgroup进行资源隔离以及分配,所以在部分情况下通过nsenter命令(在util-linux包中)来对容器进行调试查看是一个很好的选择 如果想详细关于容器和命名空间相关的内容可以参考这两篇博文 搞懂容器技术的基石： namespace （上） 搞懂容器技术的基石： namespace （下） 设置flags掩码让我们回到克隆子进程的准备工作，每个flags掩码将为子进程创建一个新的给定的命名空间。如果没有设置flag，通常子进程将成为父进程所在的命名空间的一部分。 以下是完整代码: CLONE_NEWNS将在Mount命名空间中启动克隆的子进程，挂载列表从父进程命名空间进行拷贝。查阅mount命名空间手册以获取更多信息。 CLONE_NEWCGROUP将在新的cgroup命名空间中启动克隆的子进程。稍后在教程中我们将解释cgroups，因为我们将使用它们来限制子进程的资源分配。查阅cgroup命名空间手册以获取更多信息。 CLONE_NEWPID将在新的pid命名空间中启动克隆的子进程。这意味着我们的子进程会认为它有一个PID = X，但实际上在Linux内核中它的PID为另一个。查阅pid命名空间手册以获取更多信息。 CLONE_NEWIPC将在新的ipc命名空间中启动克隆的子进程。在此命名空间内的进程可以相互交互，而在命名空间外的进程不能通过正常的IPC方法交互。查阅ipc命名空间手册以获取更多信息。 CLONE_NEWNET将在新的nerwork命名空间中启动克隆的子进程。它不会共享来自其他命名空间的接口和网络配置。查阅nerwork命名空间手册以获取更多信息。 CLONE_NEWUTS将在新的uts命名空间中启动克隆的子进程。我无法解释为什么叫UTS（UTS代表UNIX时间共享系统），但它将允许被包含的进程在命名空间中设置自己的主机名和NIS域名。查阅uts命名空间手册以获取更多信息。 所以，在创建我们的子进程时，我们会它的环境与系统环境隔离，允许它修改任何它想要修改的内容（至少对于使用的命名空间来说），而不会对我们的系统造成任何影响。 从容器中生成子进程现在我们已经有了generate_child_process函数，我们可以在我们的容器的create函数中调用它，并将它返回的的pid存储在结构体字段中。 在src/container.rs中，添加以下内容： 等待子进程结束现在我们的容器包含了生成新的干净的子进程所需的一切，接下来我们修改主函数以等待子进程完成。修改src/container.rs文件： 容器使用我们给予的参数生成子进程，然后hold并等待子进程结束后才退出。 wait_child函数在src/container.rs中的定义如下： 这个函数使用了waitpid这个系统调用,以下是来自手册的解释 waitpid()系统调用会暂停调用进程的执行，直到由pid参数指定的子进程改变状态。默认情况下，waitpid()只等待已终止的子进程，但是可以通过下面描述的options参数修改这种行为。 由于我们正在等待终止，所以我们只会传递None，并且如果系统调用没有成功完成，我们会返回一个Errcode::ContainerError错误。 测试也许从一开始你就在想，为什么我们需要使用sudo来运行我们的测试。在前七步中，这并不是必要的。但是从现在开始，由于我们为子进程创建了新的命名空间，所以需要CAP_SYS_ADMIN权限（参见权限手册或者LWN的这篇文章）。 下面是我们在测试这一步时可能得到的输出： Patch for this step这一步的代码可以在github litchipi/crabcan branch “step8”中找到.前一步到这一步的原始补丁可以在此处找到"},{"title":"【译】使用rust来写一个容器(三)","date":"2023-09-09T17:46:42.000Z","url":"/2023/09/10/%E7%94%A8rust%E5%86%99%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A83/","tags":[["rust","/tags/rust/"],["Docker","/tags/Docker/"]],"categories":[["rust","/categories/rust/"]],"content":" 注 本文章为对 Litchi Pi的《Writing a container in Rust》的翻译转载，不享受任何著作权利，不用于任何商业目的，不以任何许可证进行授权，不对任何转载行为尤其是商业转载行为负责。一切权利均由原作者 Litchi Pi 保有。个人翻译能力有限，如有疑问可查看原文。 创建骨架在这篇文章中，我们将创建容器的骨架，规划这个容器应用的工作流程，并创建一个空的计算程序，随着教程的进展，我们会逐步填充(这个计算程序)所需的部分。 容器基础现在，项目的基础部分已经可以稳定运行，参数部分也已经收集并验证。接下来我们将把配置提取到一个ContainerOpts结构体中，并初始化一个执行容器工作的Container结构体。 配置让我们在新建的src/config.rs文件中,为我们容器的配置定义一个结构体 让我们来分析一下我们获取的内容： path：在容器内执行的二进制文件/可执行文件/脚本的路径。 argv：传入命令行的全部参数（包括path选项）。 这些是执行execve 系统调用所必需的，我们将用这个调用来将我们的软件限制在一个执行上下文受限的进程中。 uid：容器内用户的ID。ID为0意味着它是root用户。 在GNU/Linux上，在/etc/passwd文件中可以看到用户ID，该文件的格式为username:x:uuid:guid:comment:homedir:shell。 mount_dir：我们在容器内部作为根目录挂载的目录的路径。 随着我们需求的增加，后续会添加更多的配置。 在结构体中我们使用了CString类型，因为它在后续调用execve系统调用时会更方便。另外，由于我们的配置将与待创建的子进程共享，这个结构体（其中包含堆上存储的数据）需要能够clone，所以我们需要为结构体添加derive(Clone)属性。(请参阅《rust官方文档》中关于所有权（Ownership）与数据复制（Data Copy）章节的内容。) 译者注 Cstring是rust中的一个字符串类型,表示一个拥有的、C兼容的、中间没有nul字节的nul结尾的字符串。这种类型的作用是能够从Rust字节片或向量安全地生成一个C语言兼容的字符串。这个类型的实例是一个静态的保证,即底层字节不包含内部的0字节(“nul字符”),并且最后的字节是0(“nul结束符”)关于Cstring类型的详细信息可以查看文档 接下来让我们创建配置结构体的构造函数。 这里的操作并不复杂，我们从命令行传入的String中获取每个参数，并通过将它转换为Vec&lt;CString&gt;，然后克隆第一个参数，并在返回一个·Ok结果的同时创建这个结构体。 容器骨架现在让我们创建用于执行主要任务的Container结构体 Container结构体定义了一个唯一的config字段，其中包含了我们的配置，并实现了三个函数： new: 从命令行参数中创建ContainerOpts结构体。 create: 处理容器创建过程。 clean_exit: 在每次退出前都会被调用，以确保我们执行的操作都会被清理。现在我们先让它们保持在基础的状态，稍后再来实现他们的功能。 最后，我们需要创建一个start函数，它会从命令行获取参数，并处理从Container结构体的创建到退出的所有事情。它返回一个Result，用于在程序中发生错误时提供信息。 代码中的?操作符是用来传播错误的。let mut container = Container::new(args)?;等同于： 在这种情况下，Err的类型必须是相同的。因此如果在我们的项目中为所有错误设定一个唯一的Errcode会变得很方便，如此一来我们几乎可以在任何地方使用?操作符，并将所有错误传递到start函数。start函数在记录错误并通过我们在错误处理部分定义的exit_with_retcode函数退出进程时，会先调用 clean_exit。随后返回一个错误代码。 链接到main函数最后一步,我们需要从我们的主函数中调用start函数。在src/main.rs文件的开始处添加以下代码： 使用exit_with_retcode(container::start(args))替换exit_with_retcode(Ok(())) 进行测试后我们可以得到如下输出: Patch for this step这一步的代码可以在githublitchipi/crabcan branch “step5”中找到。从前一步到这一步的原始补丁可以在此处找到。 检查Linux内核版本这一步完全基于原始教程中的&lt;&lt;check-linux-version&gt;&gt;部分。由于我在一个更新版本的内核上进行开发，所以目前我只会检查内核版本是否为v4.8以上，以及架构是否为x86。 获取系统信息由于接下来我们需要开始和操作系统进行交互，并且获取系统信息。所以稍后我们会使用一个非常有用的crate: nix crate 我们先来检查内核版本: 在这段代码中，我们首先使用uname获取系统信息。从获取到的信息中，通过scan_fmt crate将获取内核版本读取为f32的浮点数(float)，并检查它是否至少为v4.8，然后检查机器架构是否为x86_64。 错误处理如果内核版本过低或系统架构不合规，函数将返回Errcode::NotSupported，它包含了一个表示哪一项不受支持的数字。如果scan_fmt执行失败，我们将返回Errcode::ContainerError，这是一个新的错误类型，用于表示在我们的容器中”完全不应该发生”的错误。 让我们把这些新的错误类型追加到src/errors.rs文件里 添加至流程中并进行测试我们需要使用的宏来自scan_fmt包中, 先在src/main.rs中进行引入： 译者注 在Rust 2015中，在外部预导入包中的crate不能通过use声明来直接引用，因此通常标准做法是用extern crate将那它们纳入到当前作用域。从Rust 2018开始，use声明可以直接引用外部预导入包里的crate，在代码里使用extern crate会被认为是不规范的。 在Cargo.toml文件中添加依赖: 最后，在src/container.rs中的start函数中将check_linux_version添加到流程中 我不打算再在Rust中错误处理的优雅之处这部分内容上多费笔墨，不过你可以看看我们是怎样在不添加错误处理代码的情况下将新函数添加到现有流程中的 在测试后我们可以得到如下结果: Patch for this step这一步的代码可以在github litchipi/crabcan branch “step6”中找到.前一步到这一步的原始补丁可以在此处找到"},{"title":"【译】使用rust来写一个容器(二)","date":"2023-09-01T16:00:00.000Z","url":"/2023/09/02/%E7%94%A8rust%E5%86%99%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A82/","tags":[["rust","/tags/rust/"],["Docker","/tags/Docker/"]],"categories":[["rust","/categories/rust/"]],"content":" 注 本文章为对 Litchi Pi的《Writing a container in Rust》的翻译转载，不享受任何著作权利，不用于任何商业目的，不以任何许可证进行授权，不对任何转载行为尤其是商业转载行为负责。一切权利均由原作者 Litchi Pi 保有。个人翻译能力有限，如有疑问可查看原文。 开始项目在这篇文章中,我们将为我们的实现做好准备。每个程序员都各自有不同的习惯，一些人可能会直接通过Linux的系统调用来创建一个隔离的box。而我则更喜欢创建一个干净的基础环境来做我自己的实现,我发现这么做更有利于以后的阅读和理解,并且做干净的实现总是好事。同样的，这也能提供一些rust新手感兴趣的额外的一些tips和工具,比如参数解析、错误处理和日志记录等等. 在阅读这篇文章之前,我假定你们已经安装好了rustc以及cargo,如果你没有安装这两个工具,请按照本书说明进行操作。 创建项目我猜你已经听说过Rust的吉祥物是Ferris这只又小又可爱的螃蟹了。好,让我们吧Ferris放进容器里吧！:D 我们将创建一个名为Crabcan的Rust binary项目，目标是尽可能对这个项目的不同部分进行拆分，以便于我们在代码中进行搜索和调整，以及在数月的暂停后也能对项目很好的重新理解。 首先我们运行cargo new --bin crabcan来创建这个项目。这个命令会生成一个Cargo.toml文件,我们可以通过这个文件来为项目添加描述、添加依赖项、调整Rust编译器配置。它能避免我们手动在Makefile中创建rustc命令，是一个很方便的文件。你现在可以在这个文件中修改作者的名字和邮箱、版本等，不过目前我们还不会在这个文件中添加任何依赖项。在文件夹src/中，您将放置所有源代码。不过目前只有一个写了Hello World!代码的main.rs文件在这个文件夹里。 解析参数OK,现在我们直接打开我们的项目。首先我们要从命令行获取参数。目标是在调用我们的工具时从给出的文本形式的的flag中获取配置。 命令执行例子 这个命令会调用crabcan,以root身份将mountdir挂载到容器根目录,UID为0,输出所有debug消息，并将在容器内执行命令bash。 structopt库介绍structopt库是一个用于解析来自命令行的参数非常有用的工具（底层使用了clap库）。 使用方法非常简单，我们只需要定义一个包含所有参数的结构体： structopt的详细用法和所有功能可在其文档中找到。值得注意的是，结构体中定义的参数上方的/// text部分将用作帮助信息（例：当输入crabcan --help时出现的信息）。 创建我们自己的参数解析我们需要先创建一个新文件src/cli.rs,这个文件中包含了与命令行相关的全部内容。为了在我们的项目中使用它，我们必须先将他作为一个模块来引入到我们的项目中。 我们先在src/main.rs中把文本替换为以下内容: 我们期望src/cli.rs能提供一个基础的parse_args函数,这个函数会返回包含所有我们通过命令行定义的配置的结构体。注意:由于args并没有被使用，你会看见编译器出现的对应warning 现在让我们在src/cli.rs中实现这个parse_args函数: 在这里 我们必须导入我们必需的依赖库structopt以及在标准库中的PathBuf接下来我们定义Args结构体，它包含了所有参数以及用于解析参数的信息，让我们先来看看我们需要哪些参数： debug: 用于显示调试消息或仅显示正常日志 command: 在容器内执行的命令（带参数） uid: 在容器内作为应用运行用户的userID mount_dir: 用作容器内根目录/的文件夹。 注意：这个参数会以命令行中mount的形式来传递 这些加上了宏属性(macro attribute)structopt(short, long)定义的参数，会根据字段名称来自动创建对应的short命令行参数和long命令行参数。（如字段 toto 将被定义为参数 -t 和 –toto）。 终于，我们创建了parse_args函数，它通过结构体的from_args函数(由derive(StructOpt)宏生成)中来获取命令行参数。 在为参数验证和日志初始化设置一些占位符后，我们将参数返回。 最后我们把刚刚引入的包在Cargo.toml文件中作为依赖引入: 测试我们的代码让我们使用cargo run来运行我们的代码 就是这样,我们的参数解析功能运行成功了！现在如果我们尝试运行cargo run -- --mount ./ --uid 0 --command &quot;bash&quot; --debug的话,不会出现任何错误，你也可以在我们的src/main.rs中添加println!(&quot;&#123;:?&#125;&quot;, args)来得到还不错的输出： Patch for this step这一步的代码可以在github litchipi/crabcan branch “step1”中找到.应用于cargo new --bin创建的新项目的原始补丁可以在此处找到 添加日志记录现在我们已经成功获得了用户的输入信息，让我们用某些方式来为他提供输出。其实需要简单的文本就足够满足需求，但是我希望能将调试信息与基础的输出信息以及错误信息分开。因此，尽管有很多工具可供选择，但我选择了log库和env_logger库来实现这个功能。 log库是一个非常流行的日志工具。 它提供了一个Logtrait(请参阅此处来获取trait的解释），它定义了日志记录工具必须具有的所有功能，并允许任何其他crate实现这些功能。我选择了env_logger库来实现这些功能。 我们在Cargo.toml添加下面的依赖: 添加日志记录功能日志记录工具必须设置一个详细程度等级来进行初始化，这个等级定义了是否显示调试信息、仅显示错误信息或者是完全不显示日志信息。在我们的例子中,我们希望它在默认的情况下显示普通信息,并在我们通过命令行传递了--debug标志的情况下提高debug信息的详细程度。 让我们在src/cli.rs中初始化我们的日志记录器: 诚然,将它单独作为一个函数可能并不是很有必要,但是这么做的话代码会更有可读性,不是吗？如果你对Rust代码优化感兴趣的话，你可能想内联这个函数。 译者注 关于内联函数,也可查看这篇中文翻译[Rust 中的内联]( OK，现在我们需要在从命令行获取到参数后立刻初始化日志记录,让我们在parse_args函数中 用这段代码替换之前我们保留的占位符: 记录日志信息现在一切都就绪l，让我们在终端中记录一些内容！在src/main.rs的main函数中，我们可以将获取的参数输出到info信息中。这可以通过log::info!宏来实现 log库给我们提供了error!、warn!、info!、debug!、trace!五个级别。 测试后我们将得到输出: Patch for this step这一步的代码可以在github litchipi/crabcan branch “step2”中找到.从前一步到这一步的原始补丁可以在此处找到。 准备错误处理作为通常的练习,注意错误处理会是一个好习惯。Rust这门语言在错误处理方面非常强大,以至于我们无法在错误处理上忽略或者遗漏这个特点。 Errcode 枚举让我们创建一个定义了如下enum(枚举)的src/errors.rs文件: 每次我们添加新的错误类型时，我们都会向该enum添加一个可变体(variant)。derive(Debug)允许使用&#123;:?&#125;来格式化显示enum。 我们可能希望为每个可变体显示更完整的消息，以防我们在项目中的代码和数字使我们晕头转向。为此，让我们来实现std::fmt::Display特性，定义该对象尝试在常规&#123;&#125;格式中显示时的行为。 unreachable_patterns 属性确保在match语句描述了所有可变体时，不会收到编译器的任何警告信息。 Linux返回码Linux可执行文件在退出时会返回一个数字，它描述了一切的进展情况。 返回代码 0 表示没有错误，任何其他数字都用于描述错误以及该错误的详情（基于返回代码值）。 您可以在此处找到有关特殊返回代码及其含义的表。 我们并不追求通过我们的工具来执行bash自动化脚本,只要设置一个如果有错误则返回1,没有错误则返回0的方式就可以了。 让我们在src/errors.rs文件中定义一个exit_with_errcode函数: 这个函数会使用我们在Errcode这个enum中实现的get_retcode函数提供的返回状态码来退出进程，让我们先用最简单夜色最笨的办法来实现它: Rust中的Result当一段代码无法正常工作时,我们可以通过Rust中的Result来进行错误处理(详情请参阅此处)Result&lt;T, U&gt;需要两种类型，一种是成功时返回的T类型，一种是发生错误时返回的U类型。在我们的例子中，如果出现错误，我们希望返回Errcode，如果一切顺利，则返回我们想要的任何内容。让我们看看我们怎么在parse_args函数中进行设置： 如果在程序运行过程中出现了某种错误,我们只要简单写上: Rust中的Result非常有用而且强大,一般来说。通常在任何你想要错误处理的地方使用都是个好主意,因为这是Rust中错误处理的标准方式。 OK,现在我们需要在main函数中对函数成功或者失败两种状态分别做不同的处理,让我们使用match语句来定义这两种情况下要做哪些事: 在这段代码中,如果参数解析成功,那么我们在日志中打印参数并使用OK()作为参数来调用exit_with_retcode(这个函数会简单的返回0),我们稍后会将这里作为我们放置容器的起点。如果出现了错误,我们会在日志中打印它(注意Errcode上的&#123;&#125;格式化方式 ，它将调用我们之前实现的Display trait中的 fmt 函数)然后简单地退出并返回相关的返回代码。最后一步，我们必须将src/errors.rs设置本为项目的一个模块，并在src/main.rs文件中导入exit_with_retcode函数。 在测试完成后,我们可以得到如下输出: Patch for this step这一步的代码可以在github litchipi/crabcan branch “step3”中找到.前一步到这一步的原始补丁可以在此处找到 感谢filtoid为修复此步骤中出现的错误所提的PR 校验参数在我们深入实际工作之前,我们先校验一下从命令行传入的参数，此处我们仅仅检查一下mount_dir是否存在,不过如果有我们添加了更多选项等情况,可以通过额外的检查来对这一步进行拓展。让我们用的实际参数验证代码替换src/cli.rs中的占位符： 这个条件语句检验了路径(我们在Args结构体中定义的PathBuf类型)是否存在以及这个路径是否为目录。如果不满足这个条件,我们会返回Result::Err以及带有自定义可变体ArgumentInvalid的Errcode enum ,指明错误发生在参数mount部分。我们先在src/errors.rs中定义这个可变体: 接下来我们可以在fmt函数的match语句下添加以下内容: Patch for this step这一步的代码可以在github litchipi/crabcan branch “step4”中找到.前一步到这一步的原始补丁可以在此处找到 特别感谢 @kevinji指出的此步骤中的错误 "},{"title":"【译】使用rust来写一个容器(一)","date":"2023-08-31T11:27:56.000Z","url":"/2023/08/31/%E7%94%A8rust%E5%86%99%E4%B8%80%E4%B8%AA%E5%AE%B9%E5%99%A81/","tags":[["rust","/tags/rust/"],["Docker","/tags/Docker/"]],"categories":[["rust","/categories/rust/"]],"content":" 注 本文章为对 Litchi Pi的《Writing a container in Rust》的翻译转载，不享受任何著作权利，不用于任何商业目的，不以任何许可证进行授权，不对任何转载行为尤其是商业转载行为负责。一切权利均由原作者 Litchi Pi 保有。个人翻译能力有限，如有疑问可查看原文。 容器介绍关于这个系列这个系列的博客文章的目标是理解什么是容器、它是如何工作的，我们将使用 Rust 从零开始创建和管理容器。我们会在Linux containers in 500 lines of code系列的基础上,通过rust语言来重新实现自己的container容器 。首先，为了让我们理解如何构建容器。我们先了解什么是容器以及为什么我们需要容器，然后我们再了解一下一些常见的容器。 关于这个教程这个教程可以用以下形式选择性阅读: 作为在构建有趣项目的同时熟练掌握 Rust 的编程指南。 你不需要有任何经验就可以跟随这个指南来进行编程。请将这本书添加到您浏览器的书签中，在后续过程中，它可能对您理解正在进行的事情起到重要的辅助作用。 作为如何使用 Linux 内核功能在容器中实现高级别安全性和隔离性的详细示例。 如果你的目的是了解这些信息,那么你可以直接跳过代码部分,因为在代码部分之前我就会对这些内容做出解释。请记住，在创建这个项目时我(作者)也正在学习中，所以如果文章有错误的部分也是在所难免的,如果你对文章内容存在疑问,可以通过互联网以及我在文章中提供的参考链接地址来找到答案。 这个系列主要是我用于了解 Linux 安全措施、虚拟化功能、容器化工作原理以及使用 Rust 翻译用 C 语言编写的程序并与 Linux 内核交互的能力的一种方式。 对我来说幸运的是，这方面有很多文档、视频、文章等可以供我参考。 我建议我的读者把我的文章和原始文章来回对照,以获取更加精确的解释，以及各种外链(请检查脚注) 什么是容器？ 概述容器是一个隔离的执行环境，提供要执行的软件和底层操作系统之间的抽象。 它可以看作是一个软件虚拟化进程。因此，我们基本上只需要告诉容器“嘿，在一个隔离的盒子里执行那个东西”，控制器就会创建一个看起来像系统的盒子，应用程序会在这个盒子中合理的进行配置并执行。 用途许多服务器都使用了容器化方案，因为它为 DevOps 工程师提供了极大的灵活性和可靠性。此外在容器化环境中，如果软件崩溃、资源紧张甚至被黑客入侵的情况发生也不会损害整个系统以及在其上运行的所有其他服务。注意: 目前的服务器除了容器化方案，也广泛采用了虚拟机方案。 目标Linux系统无处不在,并且掌控着数字世界,只不过即便同样是Linux系统，嵌入在汽车传感器上的Linux与桌面、Web服务器或超级计算机上的Linux也并不相同。 容器化解决了程序员的程序在微服务互联网、云以及在各种各样形式的计算机(台式机、笔记本电脑、智能手机、智能手表等智能设备)上运行的问题。 可移植性可移植性是指软件可在各种环境下无兼容性问题运行的能力。容器是实现可移植性的一种解决方案，它常被用在不修改安装系统的情况下的软件移植工作。因此，可以在不安装软件的情况下运行那些软件，并对其进行配置以适应系统；只需要运行一个已安装在系统上的容器,让它来处理软件即可。 下文来自Docker官方网站： A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.容器是一个标准的软件单位，它将代码及其所有依赖项打包在一起，使得应用程序能在一个运行环境中快速可靠地运行到另一个运行环境中。Docker 容器镜像是一个轻量级、独立的可执行软件包，包括运行应用程序所需的一切：代码、运行时、系统工具、系统库和设置。 OK,这是一个在不考虑底层兼容性问题的前提下就可以我们的软件的好办法,它可以让我们在不同的服务器、笔记本电脑甚至嵌入式设备上开发和部署服务，而不会出现各种问题。这个功能至关重要。 隔离性容器化应用程序与底层操作系统具有隔离的执行环境，类似于虚拟机。在Docker文档中也对容器和虚拟机的区别做了解释： Containers and virtual machines have similar resource isolation and allocation benefits, but function differently because containers virtualize the operating system instead of hardware.容器和虚拟机具有类似的资源隔离和分配作用，但它们的功能不同，因为容器对操作系统而非硬件做了虚拟化。 因此，我们可以以管理员权限运行软件,并且可以在不会损害我们的系统的情况下执行任何操作。尽管理论上来说这是安全的，但实际上安全将取决于很多因素。包括为了避免容器逃逸（就像我们希望避免虚拟机逃逸一样）做的实现。如果想深入了解这部分内容,有一篇很好的文章《理解 Docker 容器逃逸》可供参考。 容器化的直观展示 OK，其实这个图在很多方面都存在错误。但是我们也能直观的看出，一个系统可以通过不同的方式实现可移植性和隔离功能。实际上，CPU/SoC 硬件具有简化虚拟化的功能，Linux 内核下以及内部的软件栈也有相应的功能，这里我就不再详细讨论这些细节了。 另外有一点值得注意,我们可以通过一系列Linux的库和内核服务使用虚拟化功能，在我们这个系列的文章中也将广泛使用这些功能来进行实现。 不同的容器一般使用他们各自的不同的容器化类型,例如LXC能够执行系统级虚拟化，而Docker则实现应用程序级别的虚拟化。 容器=隔离的软件我们所说的“软件隔离”，是指一系列让软件在系统内部运行，但不允许它改变那个系统的措施。因此，我们希望可以制定规则来禁止该软件访问未经授权的文件、使用未获许可的系统功能、修改系统配置、阻碍或改变性能等。这个目的能通过很多种方式进行实现,不过总的来讲，都是通过在软件的环境中采取各种强安全措施来屏蔽软件对底层系统的感知来实现的。所以容器本质上只是通过一组安全措施与系统隔离的应用程序而已。 常见容器事实上，对于容器业界存在许多种不同的解决方案，用以创建、执行以及管理容器，我们这边只讨论最著名的两个软件：Docker 和 LXC。 你也可以去维基百科查阅更多其他容器解决方案。 DockerDocker是最流行的的容器技术,于 2013 年作为开源发布，一直是 DevOps 在微服务和云计算方面不可或缺的基础工具之一。其关键特性之一是能够以单个镜像的形式创建容器，并且可以将其存储或发布在 DockerHub 等平台上。它的使用方式和API非常高级，添加特殊配置和外围设备访问相当简单,是一个专注于运行应用程序的容器。如果你想了解更多信息，请访问其官方网站或查阅文档。 Linux Containers (LXC)LXC 曾经是Docker的骨干部分，它使用内核特性来隔离和容器化一个应用程序，而且能在不需要另一个Linux内核的情况下尽可能创建接近标准Linux发行版的运行环境。 若要获取更多信息，可以访问linuxcontainers.org网站，其中包括LXD、LXCFS和其他相关工具的信息。"},{"title":"在kind中部署kubevela目前最新版本(v1.3.0)","date":"2022-05-21T09:42:52.497Z","url":"/2022/05/21/kubevela/","tags":[["kubernetes","/tags/kubernetes/"],["DevOps","/tags/DevOps/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"准备集群下载kind 创建kind集群旧版本的node安装ingress-nginx时会找不到networking.k8s.io/v1 ,选择node版本为1.22.7,也可使用旧版nginx-ingress。 安装ingress-nginx目前最新版本为1.1.3，所以使用该版本 解决国内无法下载镜像问题由于ingress-nginx在gcr.io中,国内无法直接下载,此处我们使用docker hub上的镜像。 没有的镜像可通过向此仓库提交issue实现 修改一下deploy.yaml的镜像版本 直接apply即可 安装kubevela使用脚本一键部署KubeVela CLI 安装 KubeVela Core 安装velaUX 可视化操作界面 给velaUX启用ingress(当前版本无法直接启用,velaUX的ingress清单为旧版networking.k8s.io/v1beta1) 推荐自己写一个资源清单 获取velaUX的账号密码 另一种获取密码的方式: 注:密码为数字+字母的8位组合,如果你不幸随机到了一个全是字母的密码登录不上webui，最快的方法还是重新部署。"},{"title":"使用rust 编写PROXY_WASM插件","date":"2022-05-21T09:41:33.993Z","url":"/2022/05/21/%E4%BD%BF%E7%94%A8rust%20%E7%BC%96%E5%86%99PROXY_WASM%E6%8F%92%E4%BB%B6/","tags":[["DevOps","/tags/DevOps/"],["envoy","/tags/envoy/"]],"categories":[["gateway","/categories/gateway/"]],"content":"proxy_wasm 是一套相对通用的标准,他允许实现了这套标准的wasm插件在envoy或者apisix网关中对请求进行一些操作。 proxy_wasm目前支持多个语言的sdk,支持的sdk参考此处Proxy-Wasm · GitHub,这边使用rust来写,实际上各个语言的写法大同小异。 开发环境安装rust 初始化项目 在Cargo.toml中添加动态库编译配置和sdk依赖 代码部分初始化配置其实比较简单,基本都是固定写法 以上定义了三个结构体,实际上起作用的部分为MyFilter,MyRoot结构体用于初始化配置的读取以及参数传递 wasm启动时,将初始化一些配置,我们实际的配置则通过配置文件读取。 逻辑到这边开始就可以写具体逻辑了,一般性来说我们只需要在HttpContext的trait中实现逻辑即可。这边的操作是在返回的请求中添加了两个请求头。 编译 测试编写docker-compose文件 修改envoy.yaml 启动验证即可 附录Http请求阶段挂载表附注: grpc 也有类似方法,此处不多做讲解 HTTP 处 理 挂 载 点 触 发 时 机 挂 载 ⽅ 法 HTTP 请 求 头 处 理 阶 段 - 关 接 收 到 客 户 端 发 送 来 的 请 求 头 数 据 时 on_http_request_headers HTTP 请 求 Body 处 理 阶 段 - 关 接 收 到 客 户 端 发 送 来 的 请 求 Body 数 据 时 on_http_request_body HTTP 请 求 Trailer 处 理 阶 段 - 关 接 收 到 客 户 端 发 送 来 的 请 求 Trailer 数 据 时 on_http_request_trailers HTTP 应 答 头 处 理 阶 段 - 关 接 收 到 后 端 服 务 响 应 的 应 答 头 数 据 时 on_http_response_headers HTTP 应 答 Body 处 理 阶 段 - 关 接 收 到 后 端 服 务 响 应 的 应 答 Body 数 据 时 on_http_response_body HTTP 应 答 Trailer 处 理 阶 段 - 关 接 收 到 后 端 服 务 响 应 的 应 答 Trailer 数 据 时 on_http_response_trailers 方法汇总(仅列出部分) 分 类 方法 名 称 用途 可 以 ⽣ 效 的 挂 载 ⽅ 法 请 求 头 处 理 get_http_request_headers 获 取 客 户 端 请 求 的 全 部 请 求 头 on_http_request_headers set_http_request_headers 替 换 客 户 端 请 求 的 全 部 请 求 头 on_http_request_headers get_http_request_header 获 取 客 户 端 请 求 的 指 定 请 求 头 on_http_request_headers set_http_request_header 设置客 户 端 请 求 的 指 定 请 求 头 on_http_request_headers add_http_request_header 新 增 ⼀ 个 客 户 端 请 求 头 on_http_request_headers 请 求 Body 处 理 get_http_request_body 获 取 客 户 端 请 求 Body on_http_request_body set_http_request_body 替 换 客 户 端 请 求 Body on_http_request_body 请 求 Trailer 处 理 get_http_request_trailers 获 取 客 户 端 请 求 的 全 部 请 求 Trailer on_http_request_trailers set_http_request_trailers 替 换 客 户 端 请 求 的 全 部 请 求 Trailer on_http_request_trailers get_http_request_trailer 获 取 客 户 端 请 求 的 指 定 请 求 Trailer on_http_request_trailers set_http_request_trailer 替 换 客 户 端 请 求 的 指 定 请 求 Trailer on_http_request_trailers add_http_response_trailer 新 增 ⼀ 个 客 户 端 请 求 Trailer on_http_request_trailers 应 答 头 处 理 get_http_response_headers 获 取 后 端 响 应 的 全 部 应 答 头 on_http_response_headers set_http_response_headers 替 换 后 端 响 应 的 全 部 应 答 头 on_http_response_headers get_http_response_header 获 取 后 端 响 应 的 指 定 应 答 头 on_http_response_headers set_http_response_header 替 换 后 端 响 应 的 指 定 应 答 头 on_http_response_headers add_http_response_header 新 增 ⼀ 个 后 端 响 应 头 on_http_response_headers 应 答 Body 处 理 get_http_response_body 获 取 客 户 端 请 求 Body on_http_response_body set_http_response_body 替 换 后 端 响 应 Body on_http_response_body 应 答 Trailer 处 理 get_http_response_trailers 获 取 后 端 响 应 的 全 部 应 答 Trailer on_http_response_trailers set_http_response_trailers 替 换 后 端 响 应 的 全 部 应 答 on_http_response_trailers get_http_response_trailer 获 取 后 端 响 应 的 指 定 应 答 Trailer on_http_response_trailers set_http_response_trailer 替 换 后 端 响 应 的 指 定 应 答 Trailer on_http_response_trailers add_http_response_trailer 新 增 ⼀ 个 后 端 响 应 on_http_response_trailers HTTP 调 ⽤ dispatch_http_call 发 送 ⼀ 个 HTTP 请 求 - get_http_call_response_headers 获 取 DispatchHttpCall 请 求 响 应 的 应 答 头 - get_http_call_response_body 获 取 DispatchHttpCall 请 求 响 应 的 应 答 Body - get_http_call_response_trailers 获 取 DispatchHttpCall 请 求 响 应 的 应 答 Trailer - 直 接 响 应 send_http_response 直 接 返 回 ⼀ 个 特 定 的 HTTP 应 答 - 上 下 ⽂ 切 换 resume_http_request 恢 复 先 前 被 暂 停 的 请 求 处 理 流 程 - resume_http_response 恢 复 先 前 被 暂 停 的 应 答 处 理 流 程 - "},{"title":"python调用registry v2接口获取镜像Label","date":"2022-01-14T11:01:01.527Z","url":"/2022/01/14/%E4%BD%BF%E7%94%A8python%E6%8B%89%E5%8F%96docker%E4%BB%93%E5%BA%93%E4%B8%AD%E9%95%9C%E5%83%8F%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE/","tags":[["DevOps","/tags/DevOps/"]],"categories":[["docker","/categories/docker/"]],"content":"背景来自一个奇奇怪怪的需求,要将一些changelog放在镜像的label中,姑且简单实现一下。 获取docker tokenps： 最简单的token获取方式,从.docker/config.json中拷贝。 开玩笑,需要docker token 可以使用如下命令生成 获取认证地址相关信息docker的客户端登陆使用的是比较简单的basic auth,简单发送请求获取Basic realm中提供的信息,他提供了docker认证地址和仓库的地址以及你申请的动作(pull,push 等) 构造新的请求来获取token 使用token来获取config信息 完整代码点击这里 "},{"title":"使用faasd体验轻量级serverless服务","date":"2021-12-20T16:25:26.478Z","url":"/2021/12/21/faasd/","tags":[["FaaS","/tags/FaaS/"],["Kubernetes","/tags/Kubernetes/"]],"categories":[["FaaS","/categories/FaaS/"]],"content":"faasd简介FaaS这个词相信大家有所耳闻,而OpenFaaS就是一款相当流行的FaaS框架,提供了多个版本,可以在kubernetes上运行,也能在其他部署了容器运行时的地方运行,faasd是他的轻量级的serverless,方便到可以在你的树莓派上体验serverless。 部署faasd官方提供了脚本,实际上一键执行即可,一般不会有什么问题,如果你使用CentOS7这种比较老旧的系统的话,推荐你先升级内核再进行部署。升级内核的方法我说过很多次,不再赘述。 没错 这就部署完成了,是不是非常简单。 过一会他会弹出一些信息，你可以获取到你的用户名和密码,存放在/var/lib/faasd/secrets/下。 目前来看试了几次都没出什么问题,唯一可能造成影响的问题应该是网络问题，国内访问github不太稳定,而很多资源需要在github拉取,这个的话可以挂代理也可以自己手动去部署。 部署完成后,访问对应服务器的8080端口就可以看见openfaas的ui了。 查看容器相关信息: 基本使用faas-cli工具faasd部署完成后,就可以使用faas-cli工具了。 这个工具也可以在其他机器上使用,连接远程的openfaas(需要先执行export OPENFAAS_URL=) 因为web-ui比较简单,所以这边就不提了。 使用前需要先进行登陆操作: 简单部署一个服务测试一下： 可以看到很快就部署ok 动手部署一个自己的faas服务当然不可能自己从头开始自己做,可以看看提供的模板： 部分模板不是官方的,如我们现在准备用的rust openfaas的模板列表类似于helm的镜像仓库,它提供各种模板 拉取也比较简单: 拉取完成后可以看到当前目录下有一个template文件夹 创建faas-cli新项目: 此时目录结构: 为了做区分,稍微修改一下rust函数返回的内容（test/lib.rs),这个函数会在我们在body中发送的内容前面添加hello 并且返回。 如果你是在其他机器上远程连到faasd,并且机器上有docker，那么可以直接使用faas-cli build -f test.yaml来构建并推送镜像,这边由于是同一台机器,所以采用kaniko来构建,标准的构建上下文通过faas-cli build --shrinkwrap -f test.yml生成: PS: 这边为了省力使用了阿里云的镜像仓库,也可以使用自建的私有仓库,参考此处 推送完成后手动部署 简单测试: OK，非常完美 参考链接 "},{"title":"来自Deis工作室的新玩具-Krustlet","date":"2021-11-14T05:15:04.485Z","url":"/2021/11/14/%E6%9D%A5%E8%87%AADeis%E5%B7%A5%E4%BD%9C%E5%AE%A4%E7%9A%84%E6%96%B0%E7%8E%A9%E5%85%B7-Krustlet/","tags":[["kubernetes","/tags/kubernetes/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"简单介绍大家都知道,Kubernetes 正在成为一个统一的调度器,只要符合kubernetes的标准,那么就可以通过它来进行调度,比如说kubevirt(调度虚拟机),当然我们今天不是为了调度虚拟机来的,我们需要调度的是WASM。 Krustlet和helm同出一源,使用rust实现kubelet,用于在kubernetes中运行wasm,只不过成熟度还是相对不足,不过作为一个概念性的尝试,确实很有意思。 部署部署其实比较简单,按照官方文档按部就班的部署就可以,简单的执行一下 部署一个能用的kubernetes集群这个集群能用就行,也包括kind,minikube,microk8s等 这边图省事,就用微软官方视频里介绍的kind方案好了,同样也可用于其他的k8s集群。 kind的yaml文件(cluster.yaml): 这个yaml文件会定义一个单master双worker的kubernetes集群 简单执行: 看一下集群状态: 记一下宿主机的ip地址: 准备bootstrap文件安装一下kubectl,拷贝一份kubeconfig,用户须有权限操作kube-system的Secret并且能审批CertificateSigningRequests, 这边我们用默认的就好。 执行官方提供的脚本官方提供了一个脚本,用于帮助我们生成bootstrap.conf 这个脚本在官方仓库的script目录下 拷贝kubeconfigcp ~/.kube/config ~/.krustlet/config/kubeconfig 安装krustlet根据自己操作系统的版本选择,下载地址 krustlet 在部分系统可能存在的问题因为图顺手用的是CentOS7的系统,这个系统相对来说比较老,krustlet部署时会出现一些问题 启动krustlet 启动后会提示你kubectl certificate approve &lt;hostname&gt;-tls,我们另起一个终端, 一些问题用kind启动集群时 大概是会出现这种报错。 官方的issue也有这个问题,参考链接 无需介意,kind的集群会尝试在krustlet机器上部署kindnet。因为我们的机器没有运行容器的环境,所以尝试会一直失败,问题不大.。 运行一下官方提供的mod kubectl apply --filename= k8s.yaml的内容: 文件内容 很快这个pod就执行完任务退出了: 看一眼输出: 不过krustlet的镜像和普通的镜像不同,需要用专门的工具,如果你想从头到尾体验一遍,可以参考这个项目"},{"title":"部署一个比较靠谱的prometheus监控","date":"2021-10-26T12:21:41.745Z","url":"/2021/10/26/Promethues/","tags":[["kubernetes","/tags/kubernetes/"],["prometheus","/tags/prometheus/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"部署方案以及组件当前helm版本:3.6.3 Prometheus charts 版本: 14.11.0 grafana charts 版本：6.17.2 此处采用helm部署,helm提供的组件如下: alertmanager: 负责接收并处理来自Prometheus Server(也可以是其它的客户端程序)的告警信息。Alertmanager可以对这些告警信息进行进一步的处理，比如当接收到大量重复告警时能够消除重复的告警信息，同时对告警信息进行分组并且路由到正确的通知方，Prometheus内置了对邮件，Slack等多种通知方式的支持，同时还支持与Webhook的集成，以支持更多定制化的场景. nodeExporter: Node Exporter 是 Prometheus 官方发布的，用于采集节点的系统信息，比如 CPU，内存，磁盘和网络等信息。 pushgateway: Prometheus Pushgateway 允许将时间序列从短暂的服务级别批处理作业推送到 Prometheus 可以采集的中间作业。结合 Prometheus 的基于文本的简单展示格式，即使没有客户端库，也可以轻松集成，甚至编写 shell 脚本。 server: prometheus 主程序 从helm仓库下载helm chart 打开prometheus文件夹下的values.yaml,修改以下部分: 修改kube-metrics镜像由于k8s.gcr.io在国内无法访问,所以无法使用原有的所有镜像,据说阿里云提供gcr.io镜像仓库的镜像,但是经过实际测试,缺少大量镜像,目前的解决方案是使用bitnami/kube-state-metrics镜像替代原有的镜像。 修改prometheus/charts/kube-state-metrics/values.yaml 修改prometheus.yaml的内容(非必要)由于准备采用1 K8S for Prometheus Dashboard 20211010 编号 13105的grafana模板来展示数据。所以需要对job相应的修改,如果不准备采用此模板,可不做此项 关于节点名称的标签，因为cadvisor是使用instance，而kube-state-metrics是使用node；这样会导致grafana节点信息表格中，没有统一的字段来连接各个查询，所以需要在prometheus.yaml中增加以下内容来给cadvisor的指标复制一个node标签。 这边直接在values.yaml修改以下部分: 部署 prometheus 到集群简单执行: 部署grafana 作为dashboard拉取helm charts简单执行: 修改grafana/values.yaml, 添加grafana监控dashboard可在此处找所需的监控模板,这边使用13105 添加数据源填写prometheus的service的地址和端口 导入dashboard输入13105 点击load点击左侧面板的Search Dashboards 可找到刚才的dashboard 一些问题目前使用他直接提供的dashboard似乎会存在某些问题 如安装prometheus时需添加node的label 容器网络显示也会出现问题,会出现nodata的情况 解决方案： 编辑nodata的这部分带宽数据 修改name = ~&quot;^k8s_.*&quot; 为name=~&quot;.*&quot;"},{"title":"在私有云部署阿里云NAS存储","date":"2021-10-20T10:55:59.208Z","url":"/2021/10/20/nas/","tags":[["kubernetes","/tags/kubernetes/"],["pv","/tags/pv/"],["pvc","/tags/pvc/"],["storageclass","/tags/storageclass/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"前提条件私有云内网与阿里云的VPC互通 配置文件下载首先下载阿里云提供的alibaba-cloud-csi-driver 在deploy文件夹下,找到rbac.yaml,这个文件用于配置csi插件的rbac权限, 找到deploy/nas文件夹,其中配置文件为nas-plugin-dedicated.yaml以及nas-provisioner-dedicated.yaml 注意: 原文件夹中有两套不同的配置,根据阿里云官方的解释,在私有云采用阿里云的NAS应该使用dedicated 修改部分配置要在私有云使用阿里云的csi插件,还需要声明csi-node的nodeid,在非阿里云的机器中,我们需要额外设置hostname作为nodeid 配置方法 nas-plugin-dedicated.yaml: nas-provisioner-dedicated.yaml： 安装CSI插件 kubernetes 1.20版本后适配问题在较高版本(1.20+)的kubernetes中使用阿里云的CSI插件会出现selfLink was empty, can&#39;t make reference报错,原因是kubernetes 1.20版本后禁用了SelfLink 根据工单,阿里云官方的解释是CSI插件 1.1.4以后的版本适配1.20 当时的解决方案 在kube-apiserver 的yaml文件中添加启动参数--feature-gates=RemoveSelfLink=false,重新apply后生效 使用阿里云NAS作为storageClass 参数 描述 mountOptions 挂载NAS的options参数在mountOptions中配置，包括NFS协议版本。 volumeAs 可选subpath、filesystem，分别表示创建子目录类型的PV和文件系统类型PV。 server 表示创建子目录类型的PV时，NAS文件系统的挂载点地址。 provisioner 驱动类型。本例中取值为nasplugin.csi.alibabacloud.com，表示使用阿里云NAS CSI插件。 reclaimPolicy PV的回收策略，默认为Delete，支持Retain。Delete模式：删除PVC的时候，PV和NAS文件系统会一起删除。Retain模式：删除PVC的时候，PV和NAS文件系统不会被删除，需要您手动删除。如果数据安全性要求高，推荐使用Retain方式以免误删数据。 archiveOnDelete 表示在reclaimPolicy为Delete时，是否删除后端存储。因为NAS为共享存储，添加此选项进行双重确认。在参数parameters下配置。默认为true，表示不会真正删除目录或文件，而是将其Rename，格式为：archived-&#123;pvName&#125;.&#123;timestamp&#125;；如果是配置为false，表示会真正删除后端对应的存储资源。 "},{"title":"使用kube-vip+containerd在openstack上搭建集群","date":"2021-09-23T12:01:03.407Z","url":"/2021/09/23/kube-vip/","tags":[["kubernetes","/tags/kubernetes/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"环境准备本文档仅做演示,除搭建master外其余操作与普通k8s集群相同,故省略部分操作 当前使用机器的操作系统均为CentOS7 主机名称 ip地址 备注 K8s-master-1 10.1.12.217 master节点 K8s-master-2 10.1.12.169 master节点 api.k8s.local 10.1.12.218 master(虚拟浮动IP) 环境初始化 内核升级内核升级的原因以及版本选择请看文末QA 依赖服务安装 开启时间同步: 容器运行时Containerd较多用户可能采用tar安装的方式安装containerd,本人相对较懒,由于docker-ce中也采用containerd,故直接安装docker-ce提供的软件包即可 修改containerd配置文件： 安装k8s 初始化前准备安装完kubernetes的组件后,crictl命令就可以使用了,但是我们发现实际使用会报错，原因是没有修改containerd的sock路径 修改kubelet启动配置,配置cgroup为systemd,否则会出现hostname无法解析的错误 Kube-vip 配置 openstack配置注意! 此处非必需 openstack的网络类型下,需要手动调整网络端口,在网络–&gt; 子网-&gt;端口中找到相应机器的端口 添加额外ip地址,添加后虚拟ip即可生效 启动k8s 模板文件 Master1 初始化 Master2 加入节点 加入Node 添加cni插件 此时所有的节点都会变成ready状态 后记部分QAQ: 为什么要升级内核 A: 以CentOS7为例,它的内核版本为3.10,3.10版本内核的release日期是2013年,且不说k8s,Docker容器开始进入视野也是13年以后的事情,并且随着Docker逐步更新,对内核版本的依赖也逐渐加强,许多高级特性都需要较高版本的内核才能使用,如Docker的Linux CGroup memory 在4.0以下版本的内核表现并不稳定 Q: 内核版本如何选择 A: 同样以CentOS为例,添加了elrepo之后 通常会给出新旧两个版本的内核可供升级,个人倾向于相对老版本的内核,稳定性更佳。如现在(2021-09-23),执行查询后可看到内核版本共两个, 相对较老的版本为5.4 以下为google GKE的节点内核版本 Anthos clusters on VMware 版本 Kubernetes 版本 节点内核版本 1.8.2 v1.20.9-gke.701 5.4.0.1021.22 (ubuntu)、5.4.120-r118 (cos) 1.8.1 v1.20.8-gke.1500 5.4.0-1018.19 (ubuntu)、5.4.120-r116 (cos) 1.8.0-gke.25 1.20.5-gke.1301 5.4.0.1015.16 (ubuntu)、5.4.104-r99 (cos) 1.8.0-gke.21 1.20.5-gke.1301 5.4.0.1015.16 (ubuntu)、5.4.104-r99 (cos) 1.7.3-gke.6 v1.19.12-gke.1100 5.4.0.1021.22 (ubuntu)、5.4.120-r118 (cos) 1.7.3-gke.2 1.19.12-gke.1100 5.4.0.1021.22 (ubuntu)、5.4.120-r118 (cos) 1.7.2 1.19.10 5.4.0.1015.16 (ubuntu)、5.4.104-r99 (cos) 1.7.1 1.19.7 5.4.0.1014.15 (ubuntu)、5.4.104-r97 (cos) 1.7.0 1.19.7 5.4.0.1010.11 (ubuntu)、5.4.92 (cos) 1.6.4 1.18.20 5.4.0.1021.22 1.6.3 1.18.18 5.4.0.1014.15 1.6.2 1.18.13 5.4.0.1009.10 1.6.1 1.18.13 5.4.0.1007.8 1.6.0 1.18.6 5.4.0.1004.5 k8s版本选择参考EKS(AWS推出的kubernetes平台)每个小版本后第六个修补版本为相对稳定性强的版本 如1.14.6 1.15.6 为什么采用containerd去docker化是kubernetes发展的大势所趋,采用containerd更符合时代需要 参考链接: 木子的笔记 使用kube-vip搭建高可用kubernetes集群"},{"title":"ansible中遇上的一个小问题","date":"2021-08-31T10:42:34.000Z","url":"/2021/08/31/ansibleproblem/","tags":[["故障处理","/tags/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"],["ansible","/tags/ansible/"]],"categories":[["Linux","/categories/Linux/"]],"content":"今天在新工作的地方研究了一下自动化的playbook,刚好遇上一个很奇怪的问题,来自ansible(version=2.10.1) 问题描述由于需要利用ansible调用zabbix的JMX监控,而监控的触发器在不同环境又是不一样的，因此需要利用ansible的set_fact 模块,为不同的环境添加不同的模板。 同时ansible 需要支持自定义传参添加额外监控模板功能。以下是实现： 举个例子: 看起来很美好对吧 那就大错特错了。 实际执行起来这段playbook会被跳过 原因排查查来查去发现是因为加了with_item的原因, 由于item的默认值是一个空的列表inputParamZabbixTemplate: [] 实际上就不会对他进行操作。 解决办法解决办法其实比较简单，就是在这一步之前执行一个变量初始化操作，这样如果有item的话原有逻辑也能继续执行，同样的如果没有的话跳过，也能依照初始变量运行 "},{"title":"利用Earthly工具进行CI/CD","date":"2021-07-22T11:00:45.025Z","url":"/2021/07/22/earthly/","tags":[["DevOps","/tags/DevOps/"]],"categories":[["docker","/categories/docker/"]],"content":"Earthly工具尝试最近发现一个比较有意思的镜像构建工具Earthly,文档地址 根据Earthly工具的介绍,它相当于Dockerfile+MAkefile+Bash 简单部署根据官网文档简单试用一下： 因为各种原因，个人选择在docker:dind的容器环境中尝试使用。 启动docker:dind容器： 在容器中： 官网例子 Earthfile: hello.py: 执行build部分： 执行docker部分 Earthly + Gitlab-CI简单实践备注：此处仅做演示,故仅采用docker:dind容器来进行CI,仓库仍采用上面的python演示仓库 Earthfile .gitlab-ci.yml 等待执行完毕即可,生产环境清使用专有earthly镜像"},{"title":"在阿里云ack中部署Yapi","date":"2021-07-22T10:48:19.309Z","url":"/2021/07/22/yapi-ack/","tags":[["kubernetes","/tags/kubernetes/"],["DevOps","/tags/DevOps/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"昨天遇上的需求，需要在公网环境部署一个Yapi服务,用于接口测试。做个小记录。 参考了此处 构建Yapi镜像搜索之后发现Yapi官方并没有提供镜像,于是参考上面的文档自己制作了一个镜像,过程参考上面的文档即可。 部署mongodb这边为了方便,使用helm部署。 从helm仓库下载helm chart 修改参数这边修改了以下参数： 部署 建立Yapi账号和数据库直接进pod执行，也没啥好说的 安装Yapi我们刚才已经制作了一个Yapi的镜像，为了方便使用，将其推送到公网的的自建harbor上。 使用kustomize来部署 kustomization.yaml deployment.yaml secret.yaml service.yaml ingress.yaml 部署"},{"title":"从python脚本到skopeo 容器搬运篇(2)","date":"2021-07-21T10:39:29.874Z","url":"/2021/07/21/imagetrans/","tags":[["docker","/tags/docker/"]],"categories":[["docker","/categories/docker/"]],"content":"在生产中有时候会遇上一个比较恶心的问题，因为经常性需要去各个甲方部署，公司的平台基于docker-compose部署，另外还有大大小小各种乱七八糟几百个镜像的业务依赖，每部署一次都需要重新挑出所需的镜像对其进行导出，再将其部署。 原有方案将docker镜像通过docker save 的方式导出为tar包，然后再到需要部署的机器上导入： 问题这个方案存在两个问题，一来是docker容器通过docker save 导出后会占用大量的磁盘空间，而且费时费力，另外需要在某台机器先部署好镜像才能进行导出， 为了提高效率，拟采用新的方式进行镜像的搬运。 使用私有镜像仓库来进行搬运docker 官方镜像中存在docker registry的镜像，该镜像可比较方便的进行导入导出，并且镜像分层，可以较好的解决磁盘占用和导入时间长的问题。这个方式类似上一章中提到的harbor做镜像存储操作， 此时registry文件夹即保存的镜像， 部署： 通过python脚本简单下载： 问题这个方案虽然解决磁盘占用问题，但仍需要通过本地的docker 导出镜像。同样会带来大量的镜像。 最终方案(?)最近听说了skopeo之后发现它可以比较方便的解决这个问题，但执行起来还是有一些阻碍： 各种依赖镜像原本都是使用Dockerfile进行构建的,此时使用skopeo去获取镜像并不容易 skopeo保存的镜像仍然存在分层重复的问题，重复占用磁盘的问题仍旧存在。 解决问题： 参考了此处 搭建私有的harbor仓库不多做赘述,可以参考此处 在本地构建镜像后推送到私有harbor先构建全部的可能用到的镜像，构建过程略，在harbor建立一个image仓库，然后推送： 使用skopeo拉取镜像skopeo镜像拉取方式： 通过脚本获取需要的镜像列表，然后对其分别拉取到image目录。 此时image的目录结构如下： 转换拉取的镜像文件 导出后文件夹格式： 拉取镜像测试"},{"title":"从python脚本到skopeo 容器搬运篇(1)","date":"2021-07-19T13:53:33.395Z","url":"/2021/07/19/skopeo1/","tags":[["docker","/tags/docker/"]],"categories":[["docker","/categories/docker/"]],"content":"镜像搬运的必要性由于业务原因经常有要求对镜像分组管理的要求，但很多镜像原本是重复的,并且都是单独的镜像，无法以单独的compose或者其他形式管理，要跨机器搬运就比较困难，原有的做法是新建一个临时的harbor仓库，做一个中转。 原有的搬运操作原有的一些镜像是存储在单独的一台服务器上，以docker image形式存在的,所以只进行了打包和推送操作: 在另外一台机器上部署： skopeo的使用和操作安装参照此链接 进行安装 基本使用 登陆私有仓库 拷贝到仓库： "},{"title":"kubernetes学习笔记(tekton)---基本部署","date":"2021-07-01T16:25:07.775Z","url":"/2021/07/02/kubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(tekton)/","tags":[["kubernetes","/tags/kubernetes/"],["DevOps","/tags/DevOps/"],["tekton","/tags/tekton/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"简介Tekton是一个云原生的CI/CD构建框架。 基本概念Task: Tekton中最小的单位，一切任务都是由Task组成。 TaskRun：创建了一个TaskRun以后，每个TaskRun就会单独控制一个Pod，Task中的每一个Step都对应pod中的一个Container。 Pipeline： Pipeline由多个Task组成，多个Task串联，按顺序执行就组成了Pipeline。 PipelineRun ： PipelineRun的作用为实际执行Pipeline。 PipelineResource: PipelineResource是Pipeline上的各种输入输出资源，比如github上的源码(输入)，编译产物或者容器镜像(输出)。 ClusterTask: 是覆盖整个集群的任务，可以在集群的范围运行任务，而不是和Task一样只能在某一命名空间运行。 运行机制每当我们创建一个Pipeline，他就会读取其中的各个Task任务,执行PipelineRun时实际开始运行Pipeline，每次运行都会生成一个新的PipelineRun，它会产生对应的TaskRun,由TaskRun去产生对应的Pod，运行任务。 部署部署Tekton TekTon 国内可能存在镜像拉取失败问题，需要魔法方法解决。 部署Tekton的dashboard ，可以方便直观的管理Tekton 部署tkn工具tkn是Tekton的CLI工具，在github直接下载对应版本即可，使用tkn可以很方便的管理Tekton 基本部署就说这么多，下一节再研究基本使用"},{"title":"Docker连接harbor出现x509的处理办法","date":"2021-07-01T10:44:05.004Z","url":"/2021/07/01/Docker%E8%BF%9E%E6%8E%A5harbor%E5%87%BA%E7%8E%B0x509%E7%9A%84%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95/","tags":[["docker","/tags/docker/"],["harbor","/tags/harbor/"],["Linux","/tags/Linux/"],["故障处理","/tags/%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86/"]],"categories":[["docker","/categories/docker/"]],"content":"今天在使用tekton做gitops时出现了x509的错误，kubernetes中的pod无法访问本地的harbor，出现这个问题的原因是本地的harbor用的是自签证书，导致docker访问时不信任。 原先的解决办法： 在/etc/docker/daemon.json添加 &quot;insecure-registries&quot;:[&quot;$DOMAIN/IP&quot;] ，然后重启docker。 但是由于不想关停测试用的kubernetes，这个办法就不可行了，于是采取了其他方式。 首先是将harbor的证书拷贝到/etc/pki/ca-trust/source/anchors/下 ，发现没有作用。 然后采取了如下方式。 在/etc/docker/certs.d(需要自建)新建一个文件夹 名称为harbor域名(e.g:www.example.com ),拷贝harbor的crt证书到此处。 成功运行tekton的task"},{"title":"定时任务(cron与at)","date":"2021-06-29T11:51:20.181Z","url":"/2021/06/29/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1(cron%E4%B8%8Eat)/","tags":[["Linux","/tags/Linux/"]],"categories":[["Linux","/categories/Linux/"]],"content":"cron(定时任务)cron是最常用的定时任务类型,用于执行周期性计划 对应的服务为crond，使用前请确保此服务已运行 常用命令crontab -e 使用vi文本编辑器编辑cronjob，可通过-u指定用户执行。 crontab -l 列出所有cronjob: crontab -r 删除用户的cronjob cronjob 格式 前五个*分别代表 分、时、日、月、周，最后一列是需要执行的命令。 使用L 可代表本月最后一天/本周最后一天： e.g: 在天（月）子表达式中，“L”表示一个月的最后一天在天（星期）自表达式中，“L”表示一个星期的最后一天，也就是SAT6L”表示这个月的倒数第６天，“FRIL”表示这个月的最一个星期五在使用“L”参数时，不要指定列表或范围，因为这会导致问题 例1：需要在每周五晚上三点钟执行 /root/dump.sh 脚本 例2：需要在每月10号执行 /root/dump.sh脚本 例3：在每月月底最后一天执行 /root/dump.sh脚本 cron的权限管理/etc/cron.deny文件为cron的黑名单，在此名单中的用户无法执行cron 另外/etc/cron.allow默认不存在，为白名单，若手动创建，则黑名单失效，除白名单外的用户无法执行cron at命令与周期性任务cron不同，at命令用于执行在某一时间只执行一次的一次性任务。 使用方法： at + 选项 + 时间参数 选项 常用的时间格式：&lt;时&gt;:&lt;分&gt; &lt;年&gt;-&lt;月&gt;-&lt;日&gt; &lt;时&gt;:&lt;分&gt; MMDDYY MM/DD/YY MM.DD.YY 英文月名 日期 年份 各种简写： Midnight(00:00) Noon(12:00) Teatime(16:00), now+time(从现在起的某段时间后执行) 例1:五分钟后执行a.sh 写法2： 权限控制同cron， 文件名/etc/at.deny、/etc/at.allow"},{"title":"kubernetes学习笔记(prometheus)","date":"2021-06-28T12:19:12.945Z","url":"/2021/06/28/kubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(prometheus)/","tags":[["kubernetes","/tags/kubernetes/"],["prometheus","/tags/prometheus/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"安装prometheusRBAC授权首先使用RBAC对prometheus进行授权: prometheus存储定义StorageClass存储（需要读写权限）: 配置文件我们把配置文件放在configMap中进行管理: 部署prometheus 注:这边要采用ingress的方式访问prometheus,也可通过其他方式访问: 方式二: 127.0.0.1:9090可以访问到prometheus 暴露prometheus服务 访问测试修改本地hosts为prometheus.k8s.local 访问此地址即可。 部署GrafanaGrafana是一个相对好用的数据展示工具，部署起来也比较简单 定义存储 部署Grafana 暴露服务 部署node-exporternode-exporter用于收集节点信息 使用在Grafana的dashboard中添加data source, 填入prometheus地址 ,这边是，刷新即可看到效果"},{"title":"通过例子学rust(迷你区块链)","date":"2021-06-24T15:37:53.922Z","url":"/2021/06/24/%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E5%AD%A6rust(%E8%BF%B7%E4%BD%A0%E5%8C%BA%E5%9D%97%E9%93%BE)/","tags":[["rust","/tags/rust/"]],"categories":[["rust","/categories/rust/"]],"content":"依旧来自 导入所需的包Cargo.toml 创建结构体(blockchain.rs)交易信息交易结构体需要包含发送者，接收者以及金额，同时需要实现序列化,Debug和Clone(后续创建新块需要)特性 区块头包含时间戳,前一个区块的hash,难度,默克尔值和nonce。 其中默克尔值是一个hash值，它包含每一笔交易的hash值。 nonce是用于工作量证明的值，得到一个nonce使得区块头的hash前X位为0，其中X为难度 区块体包含区块头，交易计数以及所有的交易信息 链包含区块列表当前交易，难度，矿工地址以及奖励 为链创建方法(blockchain.rs)创建新链要创建一个全新的区块链，我们需要得到第一笔交易信息，由Root发给第一位矿工。 首先新建一个链，接收矿工地址和难度两个参数，调用generate_new_block()方法生成第一个区块，最后返回一个chain实例 修改难度和奖励创建修改难度和奖励的方法 创建新的交易创建新交易很简单，只需要将发送者和接受者信息传入Transaction结构体，最后将结构体加入到链中的交易信息里即可，返回的bool用于判断是否创建成功。 计算当前块的hash值定义last_hash方法来计算当前块的hash值。 创建新的区块首先生成区块头，传入初始化信息。原视频使用time生成时间戳,这边使用了chrono。 然后产生一笔新的交易，从Root给miner。 创建一个新的区块，将此处的新交易添加到区块的交易中，并且将历史交易记录添加到此区块，此时transaction列表长度即为交易数。 通过get_merkle方法得到默克尔值，通过proof_of_work得到nonce，最后将此block打包到链中 计算默克尔值传入当前交易的列表，取出每一笔交易 计算hash后加入merkle列表。 如果默克尔列表中有奇数个值，则复制最后一个值并将其加入默克尔列表。 从默克尔hash列表中 取出前两个交易的hash值，拼接后将调用hash()得到一个hash，重复此操作直到默克尔列表中的hash被合并为一个时pop并返回。 工作量证明接收一个区块头并且计算hash，从hash中取出一定长度的切片并且将其解析，其中长度由difficulty决定。 parse::&lt;u32&gt;()会尝试将其解析为u32类型，这边得到的是一个Result类型，我们的目的是得到一个nonce使得最终区块头的hash值中，前X位都是0，所以在得到ERROR时，我们依然将nonce+1，知道得到所需的hash为止。 最后我们就得到了一个可以将区块头的hash的前X位变成0的nonce，用于证明工作。 hash计算hash接受一个可被serde序列化的结构体，将结构体变成字符串，我们调用sha2的方法输出得到一个&amp;[u8]类型的列表，通过hex_to_string 将其转换为一个16进制字符串，这边使用std::fmt::Write实现 主函数调用(main.rs)启动初始化启动此程序时，我们需要初始化数据，开始第一笔交易以创建区块链， 使用io::stdin().read_line读取用户输入，并且将参数传给blockchain::Chain::new创建全新的区块链 循环内容打印菜单，使用match匹配用户输入，对其做相应处理，通过对应函数返回的bool来判断执行结果 效果 完整代码 点击这里 blockchain.rs main.rs "},{"title":"kubernetes学习笔记(helm)","date":"2021-06-23T10:40:28.357Z","url":"/2021/06/23/helm%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","tags":[["kubernetes","/tags/kubernetes/"],["helm","/tags/helm/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"安装helm我们可以使用以下命令安装helm 直接执行： 备注: 虽然存在包管理器安装，但官方并不建议使用 helm 基本功能1.创建新的 chart2.chart 打包成 tgz 格式3.上传 chart 到 chart 仓库或从仓库中下载 chart4.在Kubernetes集群中安装或卸载 chart5.管理用Helm安装的 chart 的发布周期 helm基本构成helm三大概念：Chart 代表着 Helm 包。它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。你可以把它看作是 Homebrew formula，Apt dpkg，或 Yum RPM 在Kubernetes 中的等价物。 Repository（仓库）是用来存放和共享 charts 的地方。它就像 Perl 的CPAN 档案库网络或是 Fedora 的软件包仓库,只不过它是供 Kubernetes 包所使用的。 Release是运行在 Kubernetes集群中的 chart 的实例。一个chart通常可以在同一个集群中安装多次。每一次安装都会创建一个新的Release。以 MySQL chart为例，如果你想在你的集群中运行两个数据库，你可以安装该chart两次。每一个数据库都会拥有它自己的release和 release name。 在了解了上述这些概念以后，我们就可以这样来解释 Helm： Helm 安装charts到 Kubernetes 集群中，每次安装都会创建一个新的release。你可以在Helm的 chart repositories中寻找新的 chart。 helm安装chart示例以nginx为例：直接执行helm install安装 查看helm chart支持的参数 以下是完整内容 修改可配置选项在config.yaml 中自定义所需的选项 删除"},{"title":"kubernetes学习笔记(storage)","date":"2021-06-23T10:39:52.444Z","url":"/2021/06/23/k8s%20%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95(3)%20pv&pvc&StorageClass/","tags":[["kubernetes","/tags/kubernetes/"],["pv","/tags/pv/"],["pvc","/tags/pvc/"],["storageclass","/tags/storageclass/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"存储相关概念pv： 持久化卷，可以使用ceph和nfs等pvc: 持久化卷声明,用于调度pv资源StorageClass： 定义存储类型，动态分配存储资源 环境准备首先建立一个nfs的server，此处不做多讲，kubernetes节点上也安装nfs服务 pv相关属性： pv相关属性包括了Capacity(存储能力)、AccessModes(访问模式)、ReclaimPolicy(回收策略)。 基本的capacity指标为storage=”存储容量”。访问模式包含以下三种：ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 注: 不同的存储方式支持的访问模式不同，请参阅相关指南。回收策略： Retain（保留）- 保留数据，需要管理员手工清理数据 Recycle（回收）- 清除 PV 中的数据，效果相当于执行 rm -rf /yourdir/* Delete（删除）- 与 PV 相连的后端存储完成 volume 的删除操作 pv的状态通常有以下几种：Available（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PV 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 pv实践接下来新建pv对象(pv1.yaml)： 应用: 创建pvc 注: pvc会自动寻找available状态的pv，无需额外声明。如果pv为2Gi，pvc为1Gi，同样会进行绑定，且pvc的容量会变成2Gi。 使用pvc作为服务的存储 nginx-service.yaml nginx-ingress.ymal StorageClass实践新建nfs-client的deployment 创建nfs-client的serviceaccount： 此处新建了一个serviceaccount同时绑定了一个clusterrole（用于声明权限） 创建sc对象 使用StorageClass 创建服务 "},{"title":"通过例子学rust(端口嗅探器)","date":"2021-06-19T06:48:35.994Z","url":"/2021/06/19/rust%E5%AD%A6%E4%B9%A0-%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%80%9A%E8%BF%87%E4%BE%8B%E5%AD%90%E5%AD%A6rust(%E7%AB%AF%E5%8F%A3%E5%97%85%E6%8E%A2%E5%99%A8)/","tags":[["rust","/tags/rust/"]],"categories":[["rust","/categories/rust/"]],"content":"简介个人推荐使用rust的clap库实现命令行程序功能 非原创,原版在这 搬运自 首先分析需求： 获取参数我们使用std::env 来获取用户输入的参数 测试一下得到的结果： 建立结构体我们用结构体来这些参数并且将其实例化 main函数中的错误处理让我们回到main函数 main中执行多线程扫描的部分 scan 函数 完整代码 完整代码 "},{"title":"rust学习笔记(泛型)","date":"2021-06-14T06:46:44.743Z","url":"/2021/06/14/rust%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E6%B3%9B%E5%9E%8B)/","tags":[["rust","/tags/rust/"]],"categories":[["rust","/categories/rust/"]],"content":"学习内容来自B站：原子之音 泛型结构体 结构体方法中的泛型 为泛型结构体中特定的类型实现方法 泛型方法 "},{"title":"kubernetes学习笔记（ingress）","date":"2021-06-09T10:34:58.207Z","url":"/2021/06/09/k8s%20%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95%EF%BC%882%EF%BC%89%20ingress/","tags":[["kubernetes","/tags/kubernetes/"],["ingress","/tags/ingress/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"上次部署完成了k8s的基本框架，现在开始部署ingress，ingress其实就是从 kuberenets 集群外部访问集群的一个入口，将外部的请求转发到集群内不同的 Service 上，其实就相当于 nginx、haproxy 等负载均衡代理服务器。 创建traefik的crd资源 创建rbac.yaml 创建cokfigmap traefik 安装前准备在部署traefik之前 还需要安装Service APIs 安装api的两种方式 网络不佳的情况下 推荐下载下所有文件 创建deploy.yaml 文件 如果需要自定义标签的话 至此 traefik部署完成，如果需要访问traefik的dashboard 部署dashboard 修改本地hosts文件 访问 即可看到dashboard"},{"title":"kubernetes学习笔记（部署）","date":"2021-06-07T16:00:00.000Z","url":"/2021/06/08/k8s%20%20%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/","tags":[["kubernetes","/tags/kubernetes/"]],"categories":[["kubernetes","/categories/kubernetes/"]],"content":"准备三台干净的服务器，系统版本： CentOS Linux release 7.9.2009 添加hosts 配置docker加速 部署master节点 部署node节点 部署cni 容器网络 也可使用flannel 如果要切换cni插件，需要先把所有节点的/etc/cni/net.d/文件清空 验证kubernetes"},{"title":"Hello World","date":"2021-06-05T06:30:57.718Z","url":"/2021/06/05/firstblog/","tags":[["杂谈","/tags/%E6%9D%82%E8%B0%88/"]],"categories":[["杂谈","/categories/%E6%9D%82%E8%B0%88/"]],"content":"各种备忘的以及乱七八糟的东西都放到这从rust,python 的学习笔记到linux,docker 等有机会大概都会在这更新 做个纪录顺便防止自己哪天忘了 hexo主题来源为说明文档地址  "}]